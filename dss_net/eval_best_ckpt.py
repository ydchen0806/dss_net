"""
æ¨ç†å’Œè¯„ä¼°è„šæœ¬ï¼ˆä¿®å¤ç‰ˆ - æ­£ç¡®å¤„ç†ä¸åŒæ¨¡å‹æ¶æ„ï¼‰
"""

import os
import sys
import yaml
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from tqdm import tqdm
import argparse
import json
from typing import Dict, List, Tuple, Optional
import warnings

warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from dataset import create_dataloaders
from model import UNetDecomposer, UNetBaseline
from visualization import create_comparison_grid


class MetricsCalculator:
    """è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨"""
    
    @staticmethod
    def mse(pred: torch.Tensor, target: torch.Tensor) -> float:
        """å‡æ–¹è¯¯å·®"""
        return torch.mean((pred - target) ** 2).item()
    
    @staticmethod
    def mae(pred: torch.Tensor, target: torch.Tensor) -> float:
        """å¹³å‡ç»å¯¹è¯¯å·®"""
        return torch.mean(torch.abs(pred - target)).item()
    
    @staticmethod
    def rmse(pred: torch.Tensor, target: torch.Tensor) -> float:
        """å‡æ–¹æ ¹è¯¯å·®"""
        return torch.sqrt(torch.mean((pred - target) ** 2)).item()
    
    @staticmethod
    def nmse(pred: torch.Tensor, target: torch.Tensor) -> float:
        """å½’ä¸€åŒ–å‡æ–¹è¯¯å·® (dB)"""
        mse = torch.mean((pred - target) ** 2)
        power = torch.mean(target ** 2)
        nmse_linear = mse / (power + 1e-10)
        return 10 * torch.log10(nmse_linear + 1e-10).item()
    
    @staticmethod
    def psnr(pred: torch.Tensor, target: torch.Tensor, max_val: float = 1.0) -> float:
        """å³°å€¼ä¿¡å™ªæ¯”"""
        mse = torch.mean((pred - target) ** 2)
        if mse == 0:
            return float('inf')
        return (20 * torch.log10(torch.tensor(max_val)) - 10 * torch.log10(mse)).item()
    
    @staticmethod
    def snr(pred: torch.Tensor, target: torch.Tensor) -> float:
        """ä¿¡å™ªæ¯” (dB)"""
        signal_power = torch.mean(target ** 2)
        noise_power = torch.mean((pred - target) ** 2)
        return 10 * torch.log10(signal_power / (noise_power + 1e-10)).item()
    
    @staticmethod
    def cosine_similarity(pred: torch.Tensor, target: torch.Tensor) -> float:
        """ä½™å¼¦ç›¸ä¼¼åº¦"""
        pred_flat = pred.flatten()
        target_flat = target.flatten()
        
        dot_product = torch.dot(pred_flat, target_flat)
        pred_norm = torch.norm(pred_flat)
        target_norm = torch.norm(target_flat)
        
        return (dot_product / (pred_norm * target_norm + 1e-10)).item()
    
    @staticmethod
    def correlation_coefficient(pred: torch.Tensor, target: torch.Tensor) -> float:
        """ç›¸å…³ç³»æ•°"""
        pred_flat = pred.flatten()
        target_flat = target.flatten()
        
        pred_mean = torch.mean(pred_flat)
        target_mean = torch.mean(target_flat)
        
        numerator = torch.sum((pred_flat - pred_mean) * (target_flat - target_mean))
        denominator = torch.sqrt(
            torch.sum((pred_flat - pred_mean) ** 2) * 
            torch.sum((target_flat - target_mean) ** 2)
        )
        
        return (numerator / (denominator + 1e-10)).item()
    
    @staticmethod
    def relative_error(pred: torch.Tensor, target: torch.Tensor) -> float:
        """ç›¸å¯¹è¯¯å·® (%)"""
        error = torch.norm(pred - target)
        norm = torch.norm(target)
        return (error / (norm + 1e-10) * 100).item()
    
    @staticmethod
    def calculate_all_metrics(pred: torch.Tensor, target: torch.Tensor, 
                              prefix: str = '') -> Dict[str, float]:
        """è®¡ç®—æ‰€æœ‰æŒ‡æ ‡"""
        metrics = {
            f'{prefix}mse': MetricsCalculator.mse(pred, target),
            f'{prefix}mae': MetricsCalculator.mae(pred, target),
            f'{prefix}rmse': MetricsCalculator.rmse(pred, target),
            f'{prefix}nmse_db': MetricsCalculator.nmse(pred, target),
            f'{prefix}psnr': MetricsCalculator.psnr(pred, target),
            f'{prefix}snr': MetricsCalculator.snr(pred, target),
            f'{prefix}cosine_sim': MetricsCalculator.cosine_similarity(pred, target),
            f'{prefix}correlation': MetricsCalculator.correlation_coefficient(pred, target),
            f'{prefix}relative_error': MetricsCalculator.relative_error(pred, target),
        }
        return metrics


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, checkpoint_path: str, device: str = 'cuda', 
                 test_loader = None):
        """
        Args:
            checkpoint_path: checkpointæ–‡ä»¶è·¯å¾„
            device: è®¾å¤‡ ('cuda' æˆ– 'cpu')
            test_loader: é¢„å…ˆåŠ è½½çš„æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼ˆå¯é€‰ï¼‰
        """
        self.checkpoint_path = Path(checkpoint_path)
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        # ğŸ†• åŠ è½½checkpointï¼ˆä½¿ç”¨è‡ªå·±çš„configï¼Œä¸ç”¨å…±äº«configï¼‰
        print(f"\nğŸ“¦ Loading checkpoint: {self.checkpoint_path}")
        self.checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
        
        # ğŸ†• ä½¿ç”¨checkpointè‡ªå·±çš„config
        self.config = self.checkpoint['config']
        
        # ç¡®å®šæ¨¡å‹ç±»å‹
        model_name = self.config['model']['name']
        is_ablation = self.config['model'].get('ablation', {}).get('enabled', False)
        self.is_baseline = (model_name == 'UNetBaseline' or is_ablation)
        
        print(f"   Model type: {'Baseline' if self.is_baseline else 'Decomposition'}")
        print(f"   Epoch: {self.checkpoint['epoch']}")
        print(f"   Best val loss: {self.checkpoint['best_val_loss']:.6f}")
        
        # æ„å»ºæ¨¡å‹
        self.model = self._build_model()
        self.model.eval()
        
        # ä½¿ç”¨æä¾›çš„æ•°æ®åŠ è½½å™¨æˆ–åˆ›å»ºæ–°çš„
        if test_loader is not None:
            print(f"\nğŸ“Š Using shared test dataloader...")
            self.test_loader = test_loader
        else:
            print(f"\nğŸ“Š Creating test dataloader...")
            _, _, self.test_loader = create_dataloaders(
                self.config, 
                rank=0, 
                world_size=1, 
                use_ddp=False
            )
        
        print(f"   Test samples: {len(self.test_loader.dataset)}")
        print(f"   Batch size: {self.test_loader.batch_size}")
        
    def _build_model(self) -> nn.Module:
        """æ„å»ºæ¨¡å‹"""
        if self.is_baseline:
            model = UNetBaseline(
                in_channels=self.config['model']['in_channels'],
                out_channels=self.config['model']['in_channels'],
                base_channels=self.config['model']['base_channels'],
                depth=self.config['model']['depth'],
                norm_type=self.config['model']['norm_type'],
                dropout=self.config['model']['dropout']
            )
        else:
            # ğŸ†• ä»configä¸­è·å–use_attentionå‚æ•°
            use_attention = self.config['model'].get('use_attention', False)
            
            model = UNetDecomposer(
                in_channels=self.config['model']['in_channels'],
                base_channels=self.config['model']['base_channels'],
                depth=self.config['model']['depth'],
                norm_type=self.config['model']['norm_type'],
                dropout=self.config['model']['dropout'],
                use_attention=use_attention
            )
        
        # åŠ è½½æƒé‡
        state_dict = self.checkpoint['model_state_dict']
        
        # å¤„ç†DDPä¿å­˜çš„æ¨¡å‹ï¼ˆå»æ‰module.å‰ç¼€ï¼‰
        if any(k.startswith('module.') for k in state_dict.keys()):
            state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
        
        model.load_state_dict(state_dict)
        model = model.to(self.device)
        
        return model
    
    def _parse_model_output(self, pred):
        """
        è§£ææ¨¡å‹è¾“å‡ºï¼Œå¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼
        
        Returns:
            (pred_static, pred_dynamic, pred_total) æˆ– (None, None, pred_total)
        """
        # æƒ…å†µ1: å­—å…¸æ ¼å¼ {'static': ..., 'dynamic': ...}
        if isinstance(pred, dict):
            if 'static' in pred and 'dynamic' in pred:
                pred_static = pred['static']
                pred_dynamic = pred['dynamic']
                pred_total = pred_static + pred_dynamic
                return pred_static, pred_dynamic, pred_total
            elif 'output' in pred:
                return None, None, pred['output']
            else:
                # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ªtensor
                for v in pred.values():
                    if isinstance(v, torch.Tensor):
                        return None, None, v
                raise ValueError(f"Cannot find tensor in dict output: {pred.keys()}")
        
        # æƒ…å†µ2: å…ƒç»„æ ¼å¼ (static, dynamic)
        elif isinstance(pred, tuple):
            if len(pred) == 2:
                pred_static, pred_dynamic = pred
                pred_total = pred_static + pred_dynamic
                return pred_static, pred_dynamic, pred_total
            elif len(pred) == 1:
                return None, None, pred[0]
            else:
                raise ValueError(f"Unexpected tuple length: {len(pred)}")
        
        # æƒ…å†µ3: ç›´æ¥è¿”å›tensor
        elif isinstance(pred, torch.Tensor):
            return None, None, pred
        
        else:
            raise ValueError(f"Unexpected model output type: {type(pred)}")
    
    @torch.no_grad()
    def evaluate(self) -> Dict:
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°"""
        print(f"\n{'='*80}")
        print("ğŸ” Evaluating on test set...")
        print(f"{'='*80}\n")
        
        all_metrics = {
            'total': [],
            'static': [] if not self.is_baseline else None,
            'dynamic': [] if not self.is_baseline else None,
        }
        
        # ç”¨äºä¿å­˜æ ·æœ¬ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        sample_inputs = []
        sample_preds = []
        sample_targets = []
        
        for batch_idx, batch in enumerate(tqdm(self.test_loader, desc="Evaluating")):
            inputs = batch['input'].to(self.device)
            target_static = batch['static'].to(self.device)
            target_dynamic = batch['dynamic'].to(self.device)
            target_total = batch['target'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            pred = self.model(inputs)
            
            # è§£ææ¨¡å‹è¾“å‡º
            pred_static, pred_dynamic, pred_total = self._parse_model_output(pred)
            
            if self.is_baseline:
                # åŸºçº¿æ¨¡å‹ï¼šåªæœ‰æ€»ä¿¡å·
                metrics_total = MetricsCalculator.calculate_all_metrics(
                    pred_total, target_total, prefix='total_'
                )
                all_metrics['total'].append(metrics_total)
                
            else:
                # åˆ†è§£æ¨¡å‹ï¼šæœ‰é™æ€ã€åŠ¨æ€å’Œæ€»ä¿¡å·
                if pred_static is not None and pred_dynamic is not None:
                    # è®¡ç®—é™æ€åˆ†é‡æŒ‡æ ‡
                    metrics_static = MetricsCalculator.calculate_all_metrics(
                        pred_static, target_static, prefix='static_'
                    )
                    all_metrics['static'].append(metrics_static)
                    
                    # è®¡ç®—åŠ¨æ€åˆ†é‡æŒ‡æ ‡
                    metrics_dynamic = MetricsCalculator.calculate_all_metrics(
                        pred_dynamic, target_dynamic, prefix='dynamic_'
                    )
                    all_metrics['dynamic'].append(metrics_dynamic)
                
                # è®¡ç®—æ€»ä¿¡å·æŒ‡æ ‡
                metrics_total = MetricsCalculator.calculate_all_metrics(
                    pred_total, target_total, prefix='total_'
                )
                all_metrics['total'].append(metrics_total)
            
            # ğŸ†• ä¿å­˜å‰å‡ ä¸ªbatchç”¨äºå¯è§†åŒ–ï¼ˆç»Ÿä¸€ä½¿ç”¨å­—å…¸æ ¼å¼ï¼‰
            if batch_idx < 2:
                sample_inputs.append(inputs.cpu())
                
                # ç»Ÿä¸€è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                if pred_static is not None and pred_dynamic is not None:
                    sample_preds.append({
                        'static': pred_static.cpu(),
                        'dynamic': pred_dynamic.cpu(),
                        'total': pred_total.cpu()
                    })
                else:
                    sample_preds.append({
                        'total': pred_total.cpu()
                    })
                
                sample_targets.append({
                    'static': target_static.cpu(),
                    'dynamic': target_dynamic.cpu(),
                    'target': target_total.cpu()
                })
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        
        for component, metrics_list in all_metrics.items():
            if metrics_list is None or len(metrics_list) == 0:
                continue
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(metrics_list)
            
            # è®¡ç®—ç»Ÿè®¡é‡
            avg_metrics[component] = {
                'mean': df.mean().to_dict(),
                'std': df.std().to_dict(),
                'min': df.min().to_dict(),
                'max': df.max().to_dict(),
            }
        
        # ä¿å­˜æ ·æœ¬
        self.sample_inputs = torch.cat(sample_inputs, dim=0) if sample_inputs else None
        self.sample_preds = sample_preds
        self.sample_targets = sample_targets
        
        return avg_metrics
    
    def print_metrics(self, metrics: Dict):
        """æ‰“å°è¯„ä¼°æŒ‡æ ‡"""
        print(f"\n{'='*80}")
        print("ğŸ“Š Evaluation Results")
        print(f"{'='*80}\n")
        
        for component, stats in metrics.items():
            print(f"\n{'='*80}")
            print(f"ğŸ“ˆ {component.upper()} Component Metrics")
            print(f"{'='*80}")
            
            try:
                # æ£€æŸ¥statsç»“æ„
                if not stats or not isinstance(stats, dict):
                    print(f"âš ï¸  No data available for {component}")
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„é”®
                required_keys = ['mean', 'std', 'min', 'max']
                if not all(key in stats for key in required_keys):
                    print(f"âš ï¸  Incomplete statistics for {component}")
                    print(f"   Available keys: {list(stats.keys())}")
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
                if not stats['mean']:
                    print(f"âš ï¸  No metrics data for {component}")
                    continue
                
                # åˆ›å»ºDataFrame
                df = pd.DataFrame(stats)
                
                # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåºï¼ˆå¦‚æœåˆ—å­˜åœ¨çš„è¯ï¼‰
                available_cols = [col for col in ['mean', 'std', 'min', 'max'] if col in df.columns]
                if available_cols:
                    df = df[available_cols]
                
                print(df.to_string())
                print()
                
            except Exception as e:
                print(f"âŒ Error printing metrics for {component}: {e}")
                import traceback
                traceback.print_exc()
    
    def save_metrics(self, metrics: Dict, output_dir: Path):
        """ä¿å­˜è¯„ä¼°æŒ‡æ ‡"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸ºJSON
        json_file = output_dir / 'test_metrics.json'
        with open(json_file, 'w') as f:
            json.dump(metrics, f, indent=2)
        print(f"âœ… Metrics saved to: {json_file}")
        
        # ä¿å­˜ä¸ºCSV
        for component, stats in metrics.items():
            if not stats:
                continue
            csv_file = output_dir / f'test_metrics_{component}.csv'
            df = pd.DataFrame(stats)
            df.to_csv(csv_file)
            print(f"âœ… {component} metrics saved to: {csv_file}")
        
        # åˆ›å»ºæ±‡æ€»è¡¨æ ¼
        summary_data = []
        for component, stats in metrics.items():
            if not stats or 'mean' not in stats:
                continue
            for metric_name, value in stats['mean'].items():
                summary_data.append({
                    'Component': component,
                    'Metric': metric_name,
                    'Mean': value,
                    'Std': stats['std'].get(metric_name, np.nan),
                    'Min': stats['min'].get(metric_name, np.nan),
                    'Max': stats['max'].get(metric_name, np.nan)
                })
        
        if summary_data:
            summary_df = pd.DataFrame(summary_data)
            summary_file = output_dir / 'test_metrics_summary.csv'
            summary_df.to_csv(summary_file, index=False)
            print(f"âœ… Summary saved to: {summary_file}")
    
    def create_visualizations(self, output_dir: Path):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print(f"\n{'='*80}")
        print("ğŸ“Š Creating Visualizations...")
        print(f"{'='*80}\n")
        
        if self.sample_inputs is None or len(self.sample_preds) == 0:
            print("âš ï¸  No samples available for visualization")
            return
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        print("   - Creating comparison grids...")
        try:
            for mode in ['magnitude', 'phase']:
                for batch_idx in range(min(2, len(self.sample_preds))):
                    inputs = self.sample_inputs[batch_idx:batch_idx+1]
                    pred_dict = self.sample_preds[batch_idx]
                    target = {
                        'static': self.sample_targets[batch_idx]['static'][0:1],
                        'dynamic': self.sample_targets[batch_idx]['dynamic'][0:1],
                        'target': self.sample_targets[batch_idx]['target'][0:1]
                    }
                    
                    # ğŸ†• æ ¹æ®is_baselineå†³å®špredæ ¼å¼
                    if self.is_baseline:
                        # åŸºçº¿æ¨¡å‹
                        pred = pred_dict['total'][0:1]
                    else:
                        # åˆ†è§£æ¨¡å‹ï¼šä¼ é€’å­—å…¸ï¼ˆåŒ…å«staticå’Œdynamicï¼‰
                        pred = {
                            'static': pred_dict['static'][0:1],
                            'dynamic': pred_dict['dynamic'][0:1]
                        }
                    
                    grid_img = create_comparison_grid(
                        inputs, pred, target,
                        is_baseline=self.is_baseline,
                        num_samples=1,
                        mode=mode
                    )
                    
                    plt.figure(figsize=(20, 12))
                    plt.imshow(grid_img)
                    plt.axis('off')
                    plt.tight_layout()
                    
                    save_path = output_dir / f'comparison_{mode}_batch{batch_idx}.png'
                    plt.savefig(save_path, dpi=150, bbox_inches='tight')
                    plt.close()
                    
                    print(f"      âœ… Saved: {save_path.name}")
        
        except Exception as e:
            print(f"      âš ï¸  Failed to create comparison grids: {e}")
            import traceback
            traceback.print_exc()
        
        print("\nâœ… Visualizations created!")


def evaluate_single_checkpoint(checkpoint_path: str, output_dir: str, 
                               device: str = 'cuda', test_loader=None):
    """è¯„ä¼°å•ä¸ªcheckpoint"""
    print(f"\n{'='*80}")
    print(f"ğŸ”¬ Evaluating Checkpoint")
    print(f"{'='*80}")
    print(f"   Checkpoint: {checkpoint_path}")
    print(f"   Output: {output_dir}")
    print(f"{'='*80}\n")
    
    # åˆ›å»ºè¯„ä¼°å™¨ï¼ˆä¸ä¼ é€’config_overrideï¼Œè®©å®ƒä½¿ç”¨è‡ªå·±çš„configï¼‰
    evaluator = ModelEvaluator(checkpoint_path, device, test_loader)
    
    # è¯„ä¼°
    metrics = evaluator.evaluate()
    
    # æ‰“å°æŒ‡æ ‡
    evaluator.print_metrics(metrics)
    
    # ä¿å­˜æŒ‡æ ‡
    output_path = Path(output_dir)
    evaluator.save_metrics(metrics, output_path)
    
    # åˆ›å»ºå¯è§†åŒ–
    evaluator.create_visualizations(output_path / 'visualizations')
    
    print(f"\n{'='*80}")
    print("âœ… Evaluation Complete!")
    print(f"{'='*80}")
    print(f"ğŸ“ Results saved to: {output_path}")
    print(f"{'='*80}\n")
    
    return metrics


def batch_evaluate_checkpoints(base_dir: str, pattern: str = 'Ablation',
                               output_dir: str = None, device: str = 'cuda'):
    """æ‰¹é‡è¯„ä¼°å¤šä¸ªcheckpointï¼ˆå…±äº«æ•°æ®åŠ è½½å™¨ï¼‰"""
    print(f"\n{'='*80}")
    print(f"ğŸ”¬ Batch Evaluation")
    print(f"{'='*80}")
    print(f"   Base directory: {base_dir}")
    print(f"   Pattern: {pattern}")
    print(f"{'='*80}\n")
    
    base_path = Path(base_dir)
    
    # è®¾ç½®é»˜è®¤è¾“å‡ºç›®å½•
    if output_dir is None:
        output_dir = base_path / 'eval_result'
        print(f"ğŸ’¡ Using default output directory: {output_dir}\n")
    
    # æŸ¥æ‰¾æ‰€æœ‰checkpoint
    checkpoints = []
    for category_dir in base_path.iterdir():
        if not category_dir.is_dir():
            continue
        
        for exp_dir in category_dir.iterdir():
            if not exp_dir.is_dir():
                continue
            
            if pattern in exp_dir.name:
                checkpoint_path = exp_dir / 'checkpoints' / 'best.pth'
                if checkpoint_path.exists():
                    checkpoints.append((exp_dir, checkpoint_path))
    
    print(f"ğŸ“Š Found {len(checkpoints)} checkpoints to evaluate\n")
    
    if not checkpoints:
        print("âŒ No checkpoints found!")
        return
    
    # åˆ›å»ºå…±äº«çš„æ•°æ®åŠ è½½å™¨ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
    print(f"\n{'='*80}")
    print("ğŸ“¦ Creating shared test dataloader (loading once for all models)...")
    print(f"{'='*80}\n")
    
    # ä»ç¬¬ä¸€ä¸ªcheckpointåŠ è½½é…ç½®
    first_checkpoint = torch.load(checkpoints[0][1], map_location='cpu')
    shared_config = first_checkpoint['config']
    
    _, _, shared_test_loader = create_dataloaders(
        shared_config,
        rank=0,
        world_size=1,
        use_ddp=False
    )
    
    print(f"\nâœ… Shared dataloader created!")
    print(f"   Test samples: {len(shared_test_loader.dataset)}")
    print(f"   This dataloader will be reused for all {len(checkpoints)} models\n")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“ Output directory created: {output_path}\n")
    
    # è¯„ä¼°æ¯ä¸ªcheckpoint
    all_results = []
    
    for idx, (exp_dir, checkpoint_path) in enumerate(checkpoints):
        print(f"\n{'='*80}")
        print(f"ğŸ“¦ Evaluating [{idx+1}/{len(checkpoints)}]: {exp_dir.parent.name}/{exp_dir.name}")
        print(f"{'='*80}\n")
        
        try:
            # åˆ›å»ºè¯¥å®éªŒçš„è¾“å‡ºç›®å½•
            exp_output = output_path / exp_dir.parent.name / exp_dir.name
            
            # ğŸ†• ä¸ä¼ é€’config_overrideï¼Œè®©æ¯ä¸ªæ¨¡å‹ä½¿ç”¨è‡ªå·±çš„config
            metrics = evaluate_single_checkpoint(
                str(checkpoint_path),
                str(exp_output),
                device,
                test_loader=shared_test_loader  # åªå…±äº«æ•°æ®åŠ è½½å™¨
            )
            
            # è®°å½•ç»“æœ
            result = {
                'category': exp_dir.parent.name,
                'experiment': exp_dir.name,
                'checkpoint': str(checkpoint_path),
            }
            
            # æ·»åŠ æŒ‡æ ‡
            for component, stats in metrics.items():
                if not stats or 'mean' not in stats:
                    continue
                for metric_name, value in stats['mean'].items():
                    result[f'{component}_{metric_name}_mean'] = value
                    result[f'{component}_{metric_name}_std'] = stats['std'].get(metric_name, np.nan)
            
            all_results.append(result)
            
            print(f"âœ… Successfully evaluated: {exp_dir.name}\n")
        
        except Exception as e:
            print(f"âŒ Failed to evaluate {exp_dir.name}: {e}\n")
            import traceback
            traceback.print_exc()
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    if all_results:
        summary_df = pd.DataFrame(all_results)
        
        # æ’åºï¼šæŒ‰total_nmse_db_meanå‡åº
        if 'total_total_nmse_db_mean' in summary_df.columns:
            summary_df = summary_df.sort_values('total_total_nmse_db_mean')
        
        summary_file = output_path / 'evaluation_summary.csv'
        summary_df.to_csv(summary_file, index=False)
        print(f"\nâœ… Summary saved to: {summary_file}")
        
        # ä¿å­˜ä¸ºJSON
        summary_json = output_path / 'evaluation_summary.json'
        with open(summary_json, 'w') as f:
            json.dump(all_results, f, indent=2)
        print(f"âœ… JSON summary saved to: {summary_json}")
        
        # æ‰“å°ç®€è¦ç»Ÿè®¡
        print(f"\n{'='*80}")
        print("ğŸ“Š Quick Summary")
        print(f"{'='*80}")
        if 'total_total_nmse_db_mean' in summary_df.columns:
            print(f"\nğŸ† Best Model:")
            best_row = summary_df.iloc[0]
            print(f"   {best_row['category']}/{best_row['experiment']}")
            print(f"   Total NMSE: {best_row['total_total_nmse_db_mean']:.2f} dB")
            
            print(f"\nğŸ“ˆ Top 5 Models:")
            for i, (idx, row) in enumerate(summary_df.head(5).iterrows()):
                print(f"   {i+1}. {row['category']}/{row['experiment'][:40]}")
                print(f"      Total NMSE: {row['total_total_nmse_db_mean']:.2f} dB")
    
    print(f"\n{'='*80}")
    print("âœ… Batch Evaluation Complete!")
    print(f"{'='*80}")
    print(f"ğŸ“ Results saved to: {output_path}")
    print(f"ğŸ“Š Data was loaded only ONCE and reused for all {len(checkpoints)} models")
    print(f"   (Each model used its own config for correct architecture)")
    print(f"{'='*80}\n")


def main():
    parser = argparse.ArgumentParser(
        description='Evaluate Trained Models',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        '--mode',
        type=str,
        choices=['single', 'batch'],
        default='batch',
        help='Evaluation mode'
    )
    
    parser.add_argument(
        '--checkpoint',
        type=str,
        default=None,
        help='Path to checkpoint file (for single mode)'
    )
    
    parser.add_argument(
        '--base_dir',
        type=str,
        default='/LSEM/user/chenyinda/code/signal_dy_static/1104/results_20251103_180526',
        help='Base directory containing experiments (for batch mode)'
    )
    
    parser.add_argument(
        '--pattern',
        type=str,
        default='Ablation',
        help='Pattern to match experiment directories (for batch mode)'
    )
    
    parser.add_argument(
        '--output_dir',
        type=str,
        default=None,
        help='Output directory for evaluation results (default: {base_dir}/eval_result)'
    )
    
    parser.add_argument(
        '--device',
        type=str,
        default='cuda',
        choices=['cuda', 'cpu'],
        help='Device to use'
    )
    
    args = parser.parse_args()
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print(f"\n{'='*80}")
    print("âš™ï¸  Configuration")
    print(f"{'='*80}")
    print(f"   Mode: {args.mode}")
    if args.mode == 'single':
        print(f"   Checkpoint: {args.checkpoint}")
    else:
        print(f"   Base directory: {args.base_dir}")
        print(f"   Pattern: {args.pattern}")
    if args.output_dir:
        print(f"   Output directory: {args.output_dir}")
    else:
        if args.mode == 'batch':
            print(f"   Output directory: {args.base_dir}/eval_result (default)")
    print(f"   Device: {args.device}")
    print(f"{'='*80}\n")
    
    if args.mode == 'single':
        if not args.checkpoint:
            print("âŒ Error: --checkpoint is required for single mode")
            print("ğŸ’¡ Example: python eval_best_ckpt.py --mode single --checkpoint /path/to/checkpoint/best.pth")
            return
        
        # å•ä¸ªæ¨¡å¼çš„é»˜è®¤è¾“å‡ºç›®å½•
        if args.output_dir is None:
            checkpoint_path = Path(args.checkpoint)
            args.output_dir = checkpoint_path.parent.parent / 'evaluation'
        
        evaluate_single_checkpoint(
            args.checkpoint,
            args.output_dir,
            args.device
        )
    
    elif args.mode == 'batch':
        # æ£€æŸ¥base_diræ˜¯å¦å­˜åœ¨
        if not Path(args.base_dir).exists():
            print(f"âŒ Error: Base directory does not exist: {args.base_dir}")
            print("ğŸ’¡ Please specify a valid --base_dir")
            return
        
        # æ‰¹é‡è¯„ä¼°
        batch_evaluate_checkpoints(
            args.base_dir,
            args.pattern,
            args.output_dir,
            args.device
        )


if __name__ == '__main__':
    main()