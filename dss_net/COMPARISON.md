# ğŸ“Š é…ç½®å‚æ•°å¯¹æ¯” - åŸç‰ˆ vs æ”¹è¿›ç‰ˆ

## æŸå¤±å‡½æ•°æƒé‡å¯¹æ¯”

| å‚æ•° | åŸé…ç½® | æ”¹è¿›é…ç½® | å˜åŒ– | åŸå›  |
|------|--------|----------|------|------|
| **static_mse** | 1.0 | 1.0 | âœ“ ä¸å˜ | ä¿æŒåŸºç¡€æƒé‡ |
| **dynamic_mse** | 1.0 | **2.0** | â¬†ï¸ **+100%** | åŠ¨æ€åˆ†é‡éœ€è¦æ›´å¤šå…³æ³¨ |
| **total_mse** | 2.0 | **3.0** | â¬†ï¸ **+50%** | æ€»é‡å»ºæ˜¯æœ€ç»ˆç›®æ ‡ |
| **static_l1** | 0.1 | **0.01** | â¬‡ï¸ **-90%** | çº¦æŸè¿‡å¼ºï¼Œå¤§å¹…å‡å¼± |
| **dynamic_nuclear** | 0.1 | **0.01** | â¬‡ï¸ **-90%** | çº¦æŸè¿‡å¼ºï¼Œå¤§å¹…å‡å¼± |
| **static_temporal** | 0.05 | **0.01** | â¬‡ï¸ **-80%** | æ—¶é—´çº¦æŸè¿‡å¼º |
| **dynamic_temporal** | 0.05 | **0.01** | â¬‡ï¸ **-80%** | æ—¶é—´çº¦æŸè¿‡å¼º |

### æ­£åˆ™åŒ–å¼ºåº¦å¯¹æ¯”

| å‚æ•° | åŸé…ç½® | æ”¹è¿›é…ç½® | å˜åŒ– |
|------|--------|----------|------|
| **sparsity_lambda** | 0.001 | **0.0001** | â¬‡ï¸ **-90%** |
| **nuclear_lambda** | 0.001 | **0.0001** | â¬‡ï¸ **-90%** |

### æ–°å¢å‚æ•°

| å‚æ•° | æ”¹è¿›é…ç½® | è¯´æ˜ |
|------|----------|------|
| **separation_weight** | **0.05** | ğŸ†• é¼“åŠ±é™æ€å’ŒåŠ¨æ€åˆ†é‡ä¸åŒ |

---

## è®­ç»ƒé…ç½®å¯¹æ¯”

| å‚æ•° | åŸé…ç½® | æ”¹è¿›é…ç½® | å˜åŒ– | åŸå›  |
|------|--------|----------|------|------|
| **learning_rate** | 5e-4 | **1e-3** | â¬†ï¸ **+100%** | åŠ é€Ÿæ”¶æ•› |
| **epochs** | 100 | **150** | â¬†ï¸ **+50%** | å……åˆ†è®­ç»ƒ |
| **warmup_epochs** | 10 | **5** | â¬‡ï¸ **-50%** | å¿«é€Ÿè¿›å…¥ä¸»è¦è®­ç»ƒ |
| **min_lr** | 5e-7 | **1e-6** | â¬†ï¸ **+100%** | æ›´ä½çš„æœ€å°å­¦ä¹ ç‡ |
| **patience** | 20 | **30** | â¬†ï¸ **+50%** | é¿å…è¿‡æ—©åœæ­¢ |
| **min_delta** | 0.0005 | **0.0001** | â¬‡ï¸ **-80%** | æ›´ä¸¥æ ¼çš„åˆ¤æ–­ |

---

## æ¨¡å‹é…ç½®å¯¹æ¯”

| å‚æ•° | åŸé…ç½® | æ”¹è¿›é…ç½® | å˜åŒ– | åŸå›  |
|------|--------|----------|------|------|
| **base_channels** | 64 | 64* | - | *å¯é€‰128 |
| **dropout** | 0.15 | **0.1** | â¬‡ï¸ **-33%** | å‡å°‘è¿‡æ‹Ÿåˆé£é™© |
| **use_attention** | âŒ | **âœ…** | ğŸ†• | å¢å¼ºç‰¹å¾æå– |

---

## æŸå¤±å‡½æ•°æ”¹è¿›

### åŸç‰ˆæŸå¤±è®¡ç®—
```python
total_loss = (
    1.0 * static_mse +
    1.0 * dynamic_mse +
    2.0 * total_mse +
    0.1 * 0.001 * static_l1 +      # = 0.0001
    0.1 * 0.001 * dynamic_nuclear + # = 0.0001
    0.05 * static_temporal +
    0.05 * dynamic_temporal
)
```

**é—®é¢˜**ï¼š
- é™æ€å’ŒåŠ¨æ€é‡å»ºæƒé‡ç›¸åŒï¼ˆä¸åˆç†ï¼‰
- æ­£åˆ™åŒ–é¡¹æƒé‡ç›¸å¯¹é‡å»ºè¿‡é«˜

### æ”¹è¿›ç‰ˆæŸå¤±è®¡ç®—
```python
reconstruction_loss = (
    1.0 * static_mse +
    2.0 * dynamic_mse +           # â¬†ï¸ åŠ¨æ€åŠ æƒ
    3.0 * total_mse               # â¬†ï¸ æ€»é‡å»ºæœ€é‡è¦
)

regularization_loss = (
    0.01 * 0.0001 * static_l1 +      # = 0.000001 â¬‡ï¸
    0.01 * 0.0001 * dynamic_nuclear + # = 0.000001 â¬‡ï¸
)

temporal_loss = (
    0.01 * static_temporal +      # â¬‡ï¸
    0.01 * dynamic_temporal        # â¬‡ï¸ æ”¹è¿›è®¡ç®—æ–¹å¼
)

separation_loss = 0.05 * correlation  # ğŸ†•

total_loss = reconstruction_loss + regularization_loss + temporal_loss + separation_loss
```

**æ”¹è¿›ç‚¹**ï¼š
1. é‡å»ºæŸå¤±å ä¸»å¯¼åœ°ä½
2. æ­£åˆ™åŒ–å‡å¼±100å€
3. æ–°å¢åˆ†ç¦»è´¨é‡åº¦é‡

---

## æ—¶é—´çº¦æŸæ”¹è¿›

### åŸç‰ˆï¼ˆè¿‡äºæ¿€è¿›ï¼‰
```python
if should_be_smooth:
    return variation  # é™æ€ï¼šæƒ©ç½šå˜åŒ–
else:
    return 1.0 / (variation + eps) - 1.0  # âŒ åŠ¨æ€ï¼šå€’æ•°æƒ©ç½š
```

**é—®é¢˜**ï¼šå€’æ•°å‡½æ•°è¿‡äºæ¿€è¿›ï¼Œå½“variationå¾ˆå°æ—¶æŸå¤±çˆ†ç‚¸

### æ”¹è¿›ç‰ˆï¼ˆæ¸©å’Œï¼‰
```python
if should_be_smooth:
    return variation  # é™æ€ï¼šæƒ©ç½šå˜åŒ–
else:
    target_variation = 0.01
    return F.relu(target_variation - variation)  # âœ… åªåœ¨å˜åŒ–å¤ªå°æ—¶æƒ©ç½š
```

**æ”¹è¿›ç‚¹**ï¼šåªåœ¨å˜åŒ–å¤ªå°ï¼ˆ< 0.01ï¼‰æ—¶è½»å¾®æƒ©ç½šï¼Œé¿å…çˆ†ç‚¸

---

## æ¨¡å‹æ¶æ„æ”¹è¿›

### åŠ¨æ€åˆ†é‡Decoder

#### åŸç‰ˆ
```python
# æ‰€æœ‰å±‚ä½¿ç”¨ç›¸åŒçš„ä¸‹é‡‡æ ·ç‡
for i in range(depth):
    self.up_dynamic.append(
        Up(ch, ch // 2, ...)  # ç»Ÿä¸€å‡åŠ
    )
```

#### æ”¹è¿›ç‰ˆ
```python
# å‰é¢å‡ å±‚ä½¿ç”¨æ›´å¤šé€šé“
for i in range(depth):
    out_ch = ch // 2
    if i < depth // 2:
        out_ch = int(out_ch * 1.5)  # â¬†ï¸ å‰é¢å±‚æ›´å®½
    self.up_dynamic.append(
        Up(ch, out_ch, ...)
    )

# ğŸ†• æ·»åŠ refinementå±‚
self.dynamic_refine = DoubleConv(...)
```

### æ–°å¢Attentionæœºåˆ¶

```python
# ğŸ†• Bottleneck attention
self.bottleneck_attention = nn.Sequential(
    nn.Conv2d(ch, ch // 8, 1),
    nn.ReLU(inplace=True),
    nn.Conv2d(ch // 8, ch, 1),
    nn.Sigmoid()
)

# ä½¿ç”¨
if self.use_attention:
    attention_weights = self.bottleneck_attention(bottleneck)
    bottleneck = bottleneck * attention_weights
```

---

## å‚æ•°é‡å¯¹æ¯”

| æ¨¡å‹ | åŸå‚æ•°é‡ | æ”¹è¿›å‚æ•°é‡ | å¢åŠ  |
|------|---------|------------|------|
| Encoder | ~2.5M | ~2.5M | - |
| Static Decoder | ~2.5M | ~2.5M | - |
| Dynamic Decoder | ~2.5M | **~3.3M** | **+32%** |
| Attention | - | **~0.1M** | ğŸ†• |
| **Total** | ~7.5M | **~8.4M** | **+12%** |

---

## è®­ç»ƒæ—¶é—´ä¼°ç®—

| é…ç½® | åŸç‰ˆ | æ”¹è¿›ç‰ˆ | å˜åŒ– |
|------|------|--------|------|
| Epochs | 100 | 150 | +50% |
| æ¯Epochæ—¶é—´ | 10åˆ†é’Ÿ | 11åˆ†é’Ÿ | +10% (å› attention) |
| **æ€»è®­ç»ƒæ—¶é—´** | ~17å°æ—¶ | **~28å°æ—¶** | +65% |

**å»ºè®®**ï¼š
- å¯ä»¥å…ˆç”¨åŸepochsæ•°(100)æµ‹è¯•æ•ˆæœ
- å¦‚æœæ•ˆæœå¥½å†å»¶é•¿åˆ°150

---

## å†…å­˜å ç”¨ä¼°ç®—

| é…ç½® | åŸç‰ˆ | æ”¹è¿›ç‰ˆ | å˜åŒ– |
|------|------|--------|------|
| æ¨¡å‹å‚æ•° | ~7.5M Ã— 4B = 30MB | ~8.4M Ã— 4B = 34MB | +13% |
| Batchæ¿€æ´» | ~2GB | ~2.2GB | +10% |
| **æ€»æ˜¾å­˜** | ~4GB | **~4.5GB** | +12% |

**å»ºè®®**ï¼šå¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥ï¼š
- å‡å°batch_size: 192 â†’ 12 æˆ– 8
- æˆ–ä¸å¯ç”¨attention: `use_attention: false`

---

## å¿«é€ŸéªŒè¯æ–¹æ¡ˆ

### æœ€å°æ”¹åŠ¨éªŒè¯ï¼ˆæ¨èå…ˆå°è¯•ï¼‰

åªæ”¹æŸå¤±æƒé‡ï¼Œä¸æ”¹æ¨¡å‹ï¼š
```yaml
# åœ¨åŸconfig.yamlä¸­åªä¿®æ”¹è¿™äº›ï¼š
loss:
  weights:
    dynamic_mse: 2.0      # æ”¹è¿™ä¸ª
    total_mse: 3.0        # æ”¹è¿™ä¸ª
    static_l1: 0.01       # æ”¹è¿™ä¸ª
    dynamic_nuclear: 0.01 # æ”¹è¿™ä¸ª
  sparsity_lambda: 0.0001 # æ”¹è¿™ä¸ª
  nuclear_lambda: 0.0001  # æ”¹è¿™ä¸ª

training:
  learning_rate: 1.0e-3   # æ”¹è¿™ä¸ª
```

**é¢„æœŸæ•ˆæœ**ï¼š
- è®­ç»ƒ1-2å°æ—¶ålossåº”è¯¥æ˜æ˜¾ä¸‹é™
- Total NMSEåº”è¯¥æ¥è¿‘æˆ–ä¼˜äºbaseline

å¦‚æœè¿™ä¸ªéªŒè¯æˆåŠŸï¼Œå†é‡‡ç”¨å®Œæ•´æ”¹è¿›ç‰ˆæœ¬ã€‚

---

## æ€»ç»“

**æ ¸å¿ƒæ”¹è¿›**ï¼š
1. ğŸ”¥ **æŸå¤±æƒé‡é‡æ–°å¹³è¡¡**ï¼ˆæœ€å…³é”®ï¼‰
2. ğŸ”¥ **æ­£åˆ™åŒ–å¤§å¹…å‡å¼±**ï¼ˆè§£å†³è¿‡çº¦æŸï¼‰
3. ğŸ”¥ **å­¦ä¹ ç‡æé«˜**ï¼ˆåŠ é€Ÿæ”¶æ•›ï¼‰
4. â­ **æ¨¡å‹å®¹é‡å¢åŠ **ï¼ˆæå‡ä¸Šé™ï¼‰
5. â­ **è®­ç»ƒæ—¶é—´å»¶é•¿**ï¼ˆå……åˆ†å­¦ä¹ ï¼‰

**èµ„æºéœ€æ±‚**ï¼š
- æ˜¾å­˜ï¼š+0.5GBï¼ˆå¯é€šè¿‡å‡å°batch_sizeè°ƒæ•´ï¼‰
- è®­ç»ƒæ—¶é—´ï¼š+65%ï¼ˆå¯å…ˆç”¨100 epochsæµ‹è¯•ï¼‰
- ç£ç›˜ï¼šæ— æ˜¾è‘—å˜åŒ–

**é¢„æœŸæå‡**ï¼š
- Total NMSE: -24.81 â†’ **-25+** dB
- Dynamic NMSE: -12.81 â†’ **-20+** dB
- Total Loss: 0.363 â†’ **<0.01**
