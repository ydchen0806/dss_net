"""
æ¶ˆèå®éªŒç»“æœæ”¶é›†å’Œåˆ†æè„šæœ¬ï¼ˆå¢å¼ºç‰ˆï¼‰
è‡ªåŠ¨è¯»å–æ‰€æœ‰æ¶ˆèå®éªŒçš„ç»“æœå¹¶ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
æ”¯æŒæå–checkpointä¸­çš„æ‰€æœ‰å¯ç”¨ä¿¡æ¯
"""

import os
import torch
import yaml
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Any
import numpy as np
import argparse
import json
from collections import defaultdict


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    
    Returns:
        args: å‚æ•°å‘½åç©ºé—´
    """
    parser = argparse.ArgumentParser(
        description='æ¶ˆèå®éªŒç»“æœæ”¶é›†å’Œåˆ†æè„šæœ¬ï¼ˆå¢å¼ºç‰ˆï¼‰',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # è·¯å¾„å‚æ•°
    parser.add_argument(
        '--base_dir',
        type=str,
        default='/LSEM/user/chenyinda/code/signal_dy_static/1104/results_20251104_092511',
        help='å®éªŒåŸºç¡€ç›®å½•ï¼ˆè¯»å–checkpointçš„ä½ç½®ï¼‰'
    )
    
    parser.add_argument(
        '--output_dir',
        type=str,
        default='/LSEM/user/chenyinda/code/signal_dy_static/1104/results_20251104_092511/ablation_analysis',
        help='åˆ†æç»“æœè¾“å‡ºç›®å½•'
    )
    
    parser.add_argument(
        '--csv_file',
        type=str,
        default='ablation_results.csv',
        help='CSVç»“æœæ–‡ä»¶å'
    )
    
    parser.add_argument(
        '--detailed_csv_file',
        type=str,
        default='ablation_results_detailed.csv',
        help='è¯¦ç»†CSVç»“æœæ–‡ä»¶åï¼ˆåŒ…å«æ‰€æœ‰æå–çš„ä¿¡æ¯ï¼‰'
    )
    
    parser.add_argument(
        '--latex_file',
        type=str,
        default='ablation_table.tex',
        help='LaTeXè¡¨æ ¼æ–‡ä»¶å'
    )
    
    parser.add_argument(
        '--checkpoint_keys_file',
        type=str,
        default='checkpoint_keys.json',
        help='Checkpoint keysä¿¡æ¯ä¿å­˜æ–‡ä»¶'
    )
    
    # å®éªŒåŒ¹é…å‚æ•°
    parser.add_argument(
        '--pattern',
        type=str,
        default='Ablation',
        help='å®éªŒç›®å½•åç§°åŒ¹é…æ¨¡å¼ï¼ˆåœ¨ç¬¬äºŒå±‚ç›®å½•ä¸­åŒ¹é…ï¼‰'
    )
    
    parser.add_argument(
        '--exclude_dirs',
        type=str,
        nargs='*',
        default=['ablation_analysis', '.ipynb_checkpoints', '__pycache__'],
        help='è¦æ’é™¤çš„ç¬¬ä¸€å±‚ç›®å½•åç§°åˆ—è¡¨'
    )
    
    parser.add_argument(
        '--checkpoint_name',
        type=str,
        default='best.pth',
        help='checkpointæ–‡ä»¶å'
    )
    
    # å¯è§†åŒ–å‚æ•°
    parser.add_argument(
        '--figure_dpi',
        type=int,
        default=300,
        help='å›¾è¡¨åˆ†è¾¨ç‡'
    )
    
    parser.add_argument(
        '--figure_width',
        type=float,
        default=12.0,
        help='å›¾è¡¨å®½åº¦ï¼ˆè‹±å¯¸ï¼‰'
    )
    
    parser.add_argument(
        '--figure_height',
        type=float,
        default=6.0,
        help='å›¾è¡¨é«˜åº¦ï¼ˆè‹±å¯¸ï¼‰'
    )
    
    parser.add_argument(
        '--style',
        type=str,
        default='seaborn-v0_8-darkgrid',
        choices=['seaborn-v0_8-darkgrid', 'seaborn-v0_8-whitegrid', 'ggplot', 'default'],
        help='matplotlibç»˜å›¾é£æ ¼'
    )
    
    # å…¶ä»–å‚æ•°
    parser.add_argument(
        '--float_format',
        type=str,
        default='%.6f',
        help='æµ®ç‚¹æ•°æ ¼å¼åŒ–å­—ç¬¦ä¸²'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡ºä¿¡æ¯'
    )
    
    parser.add_argument(
        '--explore_first',
        action='store_true',
        help='å…ˆæ¢ç´¢ç¬¬ä¸€ä¸ªcheckpointçš„ç»“æ„ï¼Œç„¶åè¯¢é—®æ˜¯å¦ç»§ç»­'
    )
    
    args = parser.parse_args()
    return args


def explore_nested_dict(data: Any, prefix: str = '', max_depth: int = 5, current_depth: int = 0) -> Dict[str, Any]:
    """
    é€’å½’æ¢ç´¢åµŒå¥—å­—å…¸/å¯¹è±¡çš„ç»“æ„
    
    Args:
        data: è¦æ¢ç´¢çš„æ•°æ®
        prefix: é”®çš„å‰ç¼€
        max_depth: æœ€å¤§é€’å½’æ·±åº¦
        current_depth: å½“å‰é€’å½’æ·±åº¦
    
    Returns:
        ç»“æ„ä¿¡æ¯å­—å…¸
    """
    info = {}
    
    if current_depth >= max_depth:
        return {prefix: f"<max depth reached, type: {type(data).__name__}>"}
    
    if isinstance(data, dict):
        for key, value in data.items():
            full_key = f"{prefix}.{key}" if prefix else key
            
            if isinstance(value, (dict, list, tuple)):
                info.update(explore_nested_dict(value, full_key, max_depth, current_depth + 1))
            elif isinstance(value, torch.Tensor):
                info[full_key] = f"Tensor(shape={list(value.shape)}, dtype={value.dtype})"
            elif isinstance(value, (int, float, str, bool, type(None))):
                info[full_key] = f"{type(value).__name__}: {value}"
            else:
                info[full_key] = f"<{type(value).__name__}>"
    
    elif isinstance(data, (list, tuple)):
        info[prefix] = f"{type(data).__name__}(len={len(data)})"
        if len(data) > 0 and current_depth < max_depth - 1:
            # åªæ¢ç´¢ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºç¤ºä¾‹
            info.update(explore_nested_dict(data[0], f"{prefix}[0]", max_depth, current_depth + 1))
    
    elif isinstance(data, torch.Tensor):
        info[prefix] = f"Tensor(shape={list(data.shape)}, dtype={data.dtype})"
    
    else:
        info[prefix] = f"<{type(data).__name__}>"
    
    return info


def explore_checkpoint_structure(checkpoint_path: Path) -> Dict[str, Any]:
    """
    æ¢ç´¢checkpointçš„å®Œæ•´ç»“æ„
    
    Args:
        checkpoint_path: checkpointæ–‡ä»¶è·¯å¾„
    
    Returns:
        ç»“æ„ä¿¡æ¯å­—å…¸
    """
    print(f"\n{'='*80}")
    print(f"ğŸ” æ¢ç´¢Checkpointç»“æ„: {checkpoint_path.name}")
    print(f"{'='*80}\n")
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # é¡¶å±‚keys
        print("ğŸ“‹ é¡¶å±‚Keys:")
        for key in checkpoint.keys():
            value = checkpoint[key]
            if isinstance(value, torch.Tensor):
                print(f"   - {key}: Tensor(shape={list(value.shape)}, dtype={value.dtype})")
            elif isinstance(value, dict):
                print(f"   - {key}: dict (len={len(value)})")
            elif isinstance(value, (list, tuple)):
                print(f"   - {key}: {type(value).__name__} (len={len(value)})")
            else:
                print(f"   - {key}: {type(value).__name__} = {value}")
        
        # è¯¦ç»†ç»“æ„
        print(f"\nğŸ“Š è¯¦ç»†ç»“æ„:")
        structure = explore_nested_dict(checkpoint)
        
        for key, value in sorted(structure.items()):
            print(f"   {key}: {value}")
        
        print(f"\n{'='*80}\n")
        
        return structure
    
    except Exception as e:
        print(f"âŒ æ¢ç´¢å¤±è´¥: {e}")
        return {}


def extract_value_from_nested(data: Any, key_path: str, default: Any = None) -> Any:
    """
    ä»åµŒå¥—å­—å…¸ä¸­æå–å€¼
    
    Args:
        data: æ•°æ®å­—å…¸
        key_path: é”®è·¯å¾„ï¼Œä¾‹å¦‚ "optimizer.lr"
        default: é»˜è®¤å€¼
    
    Returns:
        æå–çš„å€¼
    """
    keys = key_path.split('.')
    current = data
    
    try:
        for key in keys:
            if isinstance(current, dict):
                current = current[key]
            else:
                return default
        return current
    except (KeyError, TypeError, IndexError):
        return default


def load_checkpoint_metrics(checkpoint_path: Path, verbose: bool = False, all_keys: set = None) -> Dict:
    """
    ä»checkpointæ–‡ä»¶åŠ è½½æ‰€æœ‰å¯ç”¨æŒ‡æ ‡ï¼ˆå¢å¼ºç‰ˆï¼‰
    æ”¯æŒ val_metrics çš„å®Œæ•´å±•å¼€ä¸ç±»å‹å®‰å…¨æå–
    """
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')

        # æ”¶é›†é¡¶å±‚keys
        if all_keys is not None:
            all_keys.update(checkpoint.keys())

        if verbose:
            print(f"\nğŸ“‹ Checkpoint Keys: {list(checkpoint.keys())}")

        metrics = {
            'epoch': checkpoint.get('epoch', 0),
            'best_val_loss': checkpoint.get('best_val_loss', float('inf')),
        }

        # ä¼˜åŒ– val_metrics è§£æ
        if 'val_metrics' in checkpoint:
            val_metrics_data = checkpoint['val_metrics']

            if isinstance(val_metrics_data, dict):
                for k, v in val_metrics_data.items():
                    if isinstance(v, torch.Tensor):
                        if v.numel() == 1:
                            metrics[f'val_metrics_{k}'] = v.item()
                        else:
                            metrics[f'val_metrics_{k}'] = v.mean().item()
                    elif isinstance(v, (int, float)):
                        metrics[f'val_metrics_{k}'] = v
                    elif isinstance(v, (list, tuple)):
                        if all(isinstance(x, (int, float)) for x in v):
                            metrics[f'val_metrics_{k}'] = float(np.mean(v))
                        else:
                            metrics[f'val_metrics_{k}_len'] = len(v)
                    else:
                        metrics[f'val_metrics_{k}_type'] = type(v).__name__

            elif isinstance(val_metrics_data, torch.Tensor):
                metrics['val_metrics_tensor_shape'] = list(val_metrics_data.shape)
            else:
                metrics['val_metrics_type'] = type(val_metrics_data).__name__

            if all_keys is not None:
                for subk in (val_metrics_data.keys() if isinstance(val_metrics_data, dict) else []):
                    all_keys.add(f"val_metrics.{subk}")

        # å…¼å®¹æ—§å­—æ®µå¦‚ best_val_metrics
        if 'best_val_metrics' in checkpoint and isinstance(checkpoint['best_val_metrics'], dict):
            for k, v in checkpoint['best_val_metrics'].items():
                try:
                    metrics[f'best_val_metrics_{k}'] = float(v)
                except Exception:
                    metrics[f'best_val_metrics_{k}_type'] = type(v).__name__

        # ç»Ÿè®¡æ¨¡å‹å‚æ•°æ•°é‡
        if 'model_state_dict' in checkpoint and isinstance(checkpoint['model_state_dict'], dict):
            params = checkpoint['model_state_dict']
            metrics['model_total_params'] = sum(
                p.numel() for p in params.values() if isinstance(p, torch.Tensor)
            )
            metrics['model_param_count'] = len(params)

        # è¡¥å……åŸºç¡€é…ç½®æ ‡å¿—
        metrics['config_available'] = 'config' in checkpoint
        metrics['optimizer_available'] = 'optimizer_state_dict' in checkpoint
        metrics['scheduler_available'] = 'scheduler_state_dict' in checkpoint

        if verbose:
            print(f"   âœ… æå–åˆ° {len(metrics)} ä¸ªæŒ‡æ ‡:")
            for k, v in sorted(metrics.items()):
                print(f"      - {k}: {v}")

        return metrics

    except Exception as e:
        print(f"   âš ï¸  åŠ è½½checkpointå¤±è´¥: {checkpoint_path}")
        print(f"      é”™è¯¯: {e}")
        return None


def find_experiment_dirs(base_dir: Path, pattern: str = 'Ablation', 
                         exclude_dirs: List[str] = None, verbose: bool = False) -> List[Path]:
    """
    æŸ¥æ‰¾æ‰€æœ‰æ¶ˆèå®éªŒç›®å½•ï¼ˆä¸¤å±‚ç»“æ„ï¼‰
    è·¯å¾„æ ¼å¼: base_dir/category_dir/Ablation*/checkpoints/best.pth
    
    Args:
        base_dir: å®éªŒåŸºç¡€ç›®å½•
        pattern: ç¬¬äºŒå±‚ç›®å½•åç§°åŒ¹é…æ¨¡å¼
        exclude_dirs: è¦æ’é™¤çš„ç¬¬ä¸€å±‚ç›®å½•åç§°åˆ—è¡¨
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        å®éªŒç›®å½•åˆ—è¡¨
    """
    exp_dirs = []
    
    if exclude_dirs is None:
        exclude_dirs = []
    
    if not base_dir.exists():
        print(f"âŒ åŸºç¡€ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        return exp_dirs
    
    if verbose:
        print(f"\nğŸ” å¼€å§‹æŸ¥æ‰¾å®éªŒç›®å½•...")
        print(f"   åŸºç¡€ç›®å½•: {base_dir}")
        print(f"   åŒ¹é…æ¨¡å¼: '{pattern}'")
        print(f"   æ’é™¤ç›®å½•: {exclude_dirs}")
    
    # éå†ç¬¬ä¸€å±‚ç›®å½•
    for category_dir in base_dir.iterdir():
        if not category_dir.is_dir():
            if verbose:
                print(f"   â­ï¸  è·³è¿‡éç›®å½•: {category_dir.name}")
            continue
        
        if category_dir.name in exclude_dirs:
            if verbose:
                print(f"   ğŸš« æ’é™¤ç›®å½•: {category_dir.name}")
            continue
        
        if verbose:
            print(f"\n   ğŸ“‚ æ£€æŸ¥ç±»åˆ«ç›®å½•: {category_dir.name}")
        
        # éå†ç¬¬äºŒå±‚ç›®å½•
        for exp_dir in category_dir.iterdir():
            if not exp_dir.is_dir():
                continue
            
            if pattern in exp_dir.name:
                checkpoint_path = exp_dir / 'checkpoints' / 'best.pth'
                if checkpoint_path.exists():
                    exp_dirs.append(exp_dir)
                    if verbose:
                        print(f"      âœ… æ‰¾åˆ°å®éªŒ: {exp_dir.name}")
                else:
                    if verbose:
                        print(f"      âš ï¸  {exp_dir.name} ç¼ºå°‘ checkpoints/best.pth")
            else:
                if verbose:
                    print(f"      â­ï¸  {exp_dir.name} ä¸åŒ¹é…æ¨¡å¼ '{pattern}'")
    
    if verbose:
        print(f"\nğŸ“Š æ€»å…±æ‰¾åˆ° {len(exp_dirs)} ä¸ªæœ‰æ•ˆå®éªŒç›®å½•\n")
    
    return sorted(exp_dirs)


def extract_experiment_info(exp_dir: Path, verbose: bool = False) -> Dict:
    """
    ä»å®éªŒç›®å½•æå–ä¿¡æ¯
    
    Args:
        exp_dir: å®éªŒç›®å½•è·¯å¾„
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        info: å®éªŒä¿¡æ¯å­—å…¸
    """
    info = {
        'name': exp_dir.name,
        'category': exp_dir.parent.name,
        'path': str(exp_dir),
    }
    
    # è¯»å–experiment_info.yaml
    info_file = exp_dir / 'experiment_info.yaml'
    if info_file.exists():
        try:
            with open(info_file, 'r', encoding='utf-8') as f:
                exp_info = yaml.safe_load(f)
                info.update(exp_info)
                if verbose:
                    print(f"   âœ… è¯»å–experiment_info.yaml")
        except Exception as e:
            if verbose:
                print(f"   âš ï¸  è¯»å–experiment_info.yamlå¤±è´¥: {e}")
    
    # è¯»å–config.yaml
    config_file = exp_dir / 'config.yaml'
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                info['config'] = config
                if verbose:
                    print(f"   âœ… è¯»å–config.yaml")
        except Exception as e:
            if verbose:
                print(f"   âš ï¸  è¯»å–config.yamlå¤±è´¥: {e}")
    
    return info


def collect_all_results(args) -> tuple:
    """
    æ”¶é›†æ‰€æœ‰æ¶ˆèå®éªŒçš„ç»“æœ
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    
    Returns:
        (results_df, detailed_df, all_keys): åŸºç¡€ç»“æœDataFrameã€è¯¦ç»†DataFrameå’Œæ‰€æœ‰é‡åˆ°çš„keys
    """
    base_path = Path(args.base_dir)
    
    if not base_path.exists():
        print(f"âŒ å®éªŒç›®å½•ä¸å­˜åœ¨: {args.base_dir}")
        return None, None, None
    
    # æŸ¥æ‰¾æ‰€æœ‰å®éªŒ
    exp_dirs = find_experiment_dirs(base_path, args.pattern, args.exclude_dirs, args.verbose)
    
    if not exp_dirs:
        print(f"âš ï¸  æœªæ‰¾åˆ°ä»»ä½•å®éªŒç›®å½•")
        print(f"   åŒ¹é…æ¨¡å¼: '{args.pattern}'")
        print(f"   æ’é™¤ç›®å½•: {args.exclude_dirs}")
        print(f"\nğŸ’¡ æç¤º: è¯·æ£€æŸ¥ç›®å½•ç»“æ„æ˜¯å¦ä¸º: base_dir/*/Ablation*/checkpoints/best.pth")
        return None, None, None
    
    print(f"\nğŸ“Š æ‰¾åˆ° {len(exp_dirs)} ä¸ªå®éªŒç›®å½•:")
    for exp in exp_dirs:
        print(f"   - {exp.parent.name}/{exp.name}")
    print()
    
    # å¦‚æœå¯ç”¨explore_firstï¼Œå…ˆæ¢ç´¢ç¬¬ä¸€ä¸ªcheckpoint
    if args.explore_first and len(exp_dirs) > 0:
        first_checkpoint = exp_dirs[0] / 'checkpoints' / args.checkpoint_name
        if first_checkpoint.exists():
            explore_checkpoint_structure(first_checkpoint)
            
            response = input("ğŸ“ æ˜¯å¦ç»§ç»­æ”¶é›†æ‰€æœ‰å®éªŒç»“æœ? (y/n): ").strip().lower()
            if response != 'y':
                print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                return None, None, None
    
    # æ”¶é›†æ‰€æœ‰é‡åˆ°çš„keys
    all_keys = set()
    
    # æ”¶é›†ç»“æœ
    results = []
    detailed_results = []
    
    for exp_dir in exp_dirs:
        print(f"ğŸ” å¤„ç†å®éªŒ: {exp_dir.parent.name}/{exp_dir.name}")
        
        # æå–å®éªŒä¿¡æ¯
        info = extract_experiment_info(exp_dir, args.verbose)
        
        # æŸ¥æ‰¾best checkpoint
        checkpoint_dir = exp_dir / 'checkpoints'
        best_ckpt = checkpoint_dir / args.checkpoint_name
        
        if not best_ckpt.exists():
            print(f"   âš ï¸  æœªæ‰¾åˆ°{args.checkpoint_name}ï¼Œè·³è¿‡")
            continue
        
        # åŠ è½½æŒ‡æ ‡
        metrics = load_checkpoint_metrics(best_ckpt, args.verbose, all_keys)
        
        if metrics is None:
            continue
        
        # ç»„åˆåŸºç¡€ç»“æœ
        result = {
            'Category': info.get('category', 'Unknown'),
            'Experiment': info.get('experiment_name', exp_dir.name),
            'Model': info.get('model_name', 'Unknown'),
            'Is_Ablation': info.get('is_ablation', False),
            'Temporal': info.get('temporal_enabled', True),
            'Epoch': metrics.get('epoch', 0),
            'Best_Val_Loss': metrics.get('best_val_loss', float('inf')),
        }
        
        # ä»configæå–é¢å¤–ä¿¡æ¯
        if 'config' in info:
            config = info['config']
            result.update({
                'Use_Attention': config.get('model', {}).get('use_attention', False),
                'Separation_Weight': config.get('loss', {}).get('separation_weight', 0),
                'Dynamic_MSE_Weight': config.get('loss', {}).get('weights', {}).get('dynamic_mse', 0),
                'Learning_Rate': config.get('training', {}).get('learning_rate', 0),
            })
        
        results.append(result)
        
        # ç»„åˆè¯¦ç»†ç»“æœï¼ˆåŒ…å«æ‰€æœ‰æå–çš„metricsï¼‰
        detailed_result = result.copy()
        detailed_result.update(metrics)
        detailed_results.append(detailed_result)
        
        print(f"   âœ… å·²æ·»åŠ ç»“æœ (Loss: {result['Best_Val_Loss']:.6f}, æå–æŒ‡æ ‡æ•°: {len(metrics)})")
    
    if not results:
        print("âŒ æœªæ”¶é›†åˆ°ä»»ä½•æœ‰æ•ˆç»“æœï¼Œè¯·æ£€æŸ¥å®éªŒç›®å½•")
        return None, None, None
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(results)
    detailed_df = pd.DataFrame(detailed_results)
    
    # æ’åº
    df = df.sort_values('Best_Val_Loss')
    detailed_df = detailed_df.sort_values('Best_Val_Loss')
    
    return df, detailed_df, all_keys


def create_comparison_plots(df: pd.DataFrame, args):
    """
    åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨
    
    Args:
        df: ç»“æœDataFrame
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    output_path = Path(args.output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®ç»˜å›¾é£æ ¼
    plt.style.use(args.style)
    sns.set_palette("husl")
    
    # 1. Losså¯¹æ¯”æ¡å½¢å›¾
    fig, ax = plt.subplots(figsize=(args.figure_width, args.figure_height))
    
    labels = [f"{row['Category']}\n{row['Experiment'].replace('Ablation', 'Abl')[:20]}" 
              for _, row in df.iterrows()]
    losses = df['Best_Val_Loss'].values
    
    # é¢œè‰²æ˜ å°„
    color_map = {
        'full': 'green',
        'baseunet': 'orange',
        'baseline': 'orange',
    }
    
    colors = []
    for category in df['Category'].values:
        category_lower = category.lower()
        color = 'skyblue'
        for key, val in color_map.items():
            if key in category_lower:
                color = val
                break
        colors.append(color)
    
    bars = ax.bar(range(len(labels)), losses, color=colors, alpha=0.7, edgecolor='black')
    ax.set_xticks(range(len(labels)))
    ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=8)
    ax.set_ylabel('Validation Loss', fontsize=12, fontweight='bold')
    ax.set_title('Ablation Study - Loss Comparison', fontsize=14, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, loss) in enumerate(zip(bars, losses)):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{loss:.4f}',
                ha='center', va='bottom', fontsize=7, rotation=0)
    
    plt.tight_layout()
    loss_comparison_file = output_path / 'loss_comparison.png'
    plt.savefig(loss_comparison_file, dpi=args.figure_dpi, bbox_inches='tight')
    print(f"âœ… ä¿å­˜å›¾è¡¨: {loss_comparison_file}")
    plt.close()
    
    # 2. æŒ‰ç±»åˆ«åˆ†ç»„çš„Losså¯¹æ¯”å›¾
    fig, ax = plt.subplots(figsize=(args.figure_width, args.figure_height))
    
    categories = df['Category'].unique()
    x_pos = 0
    xticks = []
    xticklabels = []
    
    for category in categories:
        category_df = df[df['Category'] == category]
        n_exps = len(category_df)
        
        positions = range(x_pos, x_pos + n_exps)
        losses = category_df['Best_Val_Loss'].values
        
        color = color_map.get(category.lower(), 'skyblue')
        bars = ax.bar(positions, losses, color=color, alpha=0.7, 
                      edgecolor='black', label=category)
        
        for pos, loss in zip(positions, losses):
            ax.text(pos, loss, f'{loss:.4f}',
                   ha='center', va='bottom', fontsize=7)
        
        xticks.extend(positions)
        xticklabels.extend([exp.replace('Ablation', 'Abl')[:15] 
                           for exp in category_df['Experiment'].values])
        
        x_pos += n_exps + 1
    
    ax.set_xticks(xticks)
    ax.set_xticklabels(xticklabels, rotation=45, ha='right', fontsize=8)
    ax.set_ylabel('Validation Loss', fontsize=12, fontweight='bold')
    ax.set_title('Ablation Study - Loss by Category', fontsize=14, fontweight='bold')
    ax.legend(loc='upper right')
    ax.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    category_comparison_file = output_path / 'loss_by_category.png'
    plt.savefig(category_comparison_file, dpi=args.figure_dpi, bbox_inches='tight')
    print(f"âœ… ä¿å­˜å›¾è¡¨: {category_comparison_file}")
    plt.close()
    
    # 3. å¦‚æœæœ‰æ›´å¤šæŒ‡æ ‡ï¼Œåˆ›å»ºé¢å¤–çš„å¯¹æ¯”å›¾
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    metric_columns = [col for col in numeric_columns 
                     if col not in ['Epoch', 'Is_Ablation']]
    
    if len(metric_columns) > 2:
        # åˆ›å»ºå¤šæŒ‡æ ‡å¯¹æ¯”å›¾
        n_metrics = min(len(metric_columns), 6)  # æœ€å¤šæ˜¾ç¤º6ä¸ªæŒ‡æ ‡
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.flatten()
        
        for idx, metric in enumerate(metric_columns[:n_metrics]):
            ax = axes[idx]
            
            # è¿‡æ»¤æ‰NaNå€¼
            valid_data = df[df[metric].notna()]
            if len(valid_data) == 0:
                continue
            
            labels = [f"{row['Category'][:10]}\n{row['Experiment'][:15]}" 
                     for _, row in valid_data.iterrows()]
            values = valid_data[metric].values
            
            ax.bar(range(len(labels)), values, alpha=0.7, edgecolor='black')
            ax.set_xticks(range(len(labels)))
            ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=6)
            ax.set_ylabel(metric, fontsize=10)
            ax.set_title(f'{metric} Comparison', fontsize=11)
            ax.grid(axis='y', alpha=0.3)
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(n_metrics, len(axes)):
            axes[idx].set_visible(False)
        
        plt.tight_layout()
        multi_metric_file = output_path / 'multi_metric_comparison.png'
        plt.savefig(multi_metric_file, dpi=args.figure_dpi, bbox_inches='tight')
        print(f"âœ… ä¿å­˜å›¾è¡¨: {multi_metric_file}")
        plt.close()
    
    print(f"\nğŸ“Š æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")


def generate_latex_table(df: pd.DataFrame, args):
    """
    ç”ŸæˆLaTeXæ ¼å¼çš„è¡¨æ ¼
    
    Args:
        df: ç»“æœDataFrame
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    output_path = Path(args.output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ç®€åŒ–åˆ—å
    columns_to_include = ['Category', 'Experiment', 'Model', 'Best_Val_Loss', 'Epoch']
    columns_to_include = [col for col in columns_to_include if col in df.columns]
    
    df_latex = df[columns_to_include].copy()
    
    # é‡å‘½ååˆ—
    column_rename = {
        'Category': 'Category',
        'Experiment': 'Configuration',
        'Model': 'Model',
        'Best_Val_Loss': 'Loss',
        'Epoch': 'Epoch'
    }
    df_latex.columns = [column_rename.get(col, col) for col in df_latex.columns]
    
    # ç”ŸæˆLaTeXä»£ç 
    latex_code = df_latex.to_latex(
        index=False,
        float_format=args.float_format,
        caption='Ablation Study Results',
        label='tab:ablation',
        escape=False
    )
    
    # ä¿å­˜
    latex_file = output_path / args.latex_file
    with open(latex_file, 'w', encoding='utf-8') as f:
        f.write(latex_code)
    
    print(f"\nâœ… LaTeXè¡¨æ ¼å·²ä¿å­˜: {latex_file}")


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    print("\n" + "="*80)
    print("ğŸ”¬ æ¶ˆèå®éªŒç»“æœæ”¶é›†ä¸åˆ†æï¼ˆå¢å¼ºç‰ˆï¼‰")
    print("="*80)
    print(f"ğŸ“ å®éªŒç›®å½•: {args.base_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ” åŒ¹é…æ¨¡å¼: '{args.pattern}' (åœ¨ç¬¬äºŒå±‚ç›®å½•ä¸­åŒ¹é…)")
    print(f"ğŸš« æ’é™¤ç›®å½•: {args.exclude_dirs}")
    print(f"ğŸ“‚ ç›®å½•ç»“æ„: base_dir/*/Ablation*/checkpoints/best.pth")
    print("="*80 + "\n")
    
    # 1. æ”¶é›†ç»“æœ
    df, detailed_df, all_keys = collect_all_results(args)
    
    if df is None or len(df) == 0:
        print("\nâŒ æœªèƒ½æ”¶é›†åˆ°æœ‰æ•ˆç»“æœï¼Œè¯·æ£€æŸ¥:")
        print("   1. å®éªŒç›®å½•è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("   2. ç›®å½•ç»“æ„æ˜¯å¦ä¸º: base_dir/category/Ablation*/checkpoints/best.pth")
        print("   3. best.pth æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        return
    
    # 2. æ‰“å°checkpoint keysä¿¡æ¯
    print("\n" + "="*80)
    print("ğŸ”‘ æ‰€æœ‰Checkpointä¸­å‘ç°çš„Keys")
    print("="*80)
    for key in sorted(all_keys):
        print(f"   - {key}")
    print("="*80 + "\n")
    
    # ä¿å­˜keysä¿¡æ¯
    output_path = Path(args.output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    keys_file = output_path / args.checkpoint_keys_file
    with open(keys_file, 'w', encoding='utf-8') as f:
        json.dump({
            'all_keys': sorted(list(all_keys)),
            'num_keys': len(all_keys)
        }, f, indent=2)
    print(f"âœ… Checkpoint keysä¿¡æ¯å·²ä¿å­˜: {keys_file}\n")
    
    # 3. æ‰“å°åŸºç¡€ç»“æœ
    print("\n" + "="*80)
    print("ğŸ“Š å®éªŒç»“æœæ±‡æ€»ï¼ˆåŸºç¡€æŒ‡æ ‡ï¼‰")
    print("="*80)
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    pd.set_option('display.max_colwidth', 30)
    print(df.to_string(index=False))
    print("="*80 + "\n")
    
    # 4. æ‰“å°è¯¦ç»†ç»“æœé¢„è§ˆ
    print("\n" + "="*80)
    print("ğŸ“Š è¯¦ç»†ç»“æœé¢„è§ˆï¼ˆåŒ…å«æ‰€æœ‰æå–çš„æŒ‡æ ‡ï¼‰")
    print("="*80)
    print(f"æ€»åˆ—æ•°: {len(detailed_df.columns)}")
    print(f"åˆ—å: {list(detailed_df.columns)}")
    print("\nå‰3è¡Œæ•°æ®:")
    print(detailed_df.head(3).to_string(index=False))
    print("="*80 + "\n")
    
    # 5. ä¿å­˜CSV
    csv_file = output_path / args.csv_file
    df.to_csv(csv_file, index=False)
    print(f"âœ… åŸºç¡€ç»“æœå·²ä¿å­˜: {csv_file}")
    
    detailed_csv_file = output_path / args.detailed_csv_file
    detailed_df.to_csv(detailed_csv_file, index=False)
    print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜: {detailed_csv_file}\n")
    
    # 6. åˆ›å»ºå¯è§†åŒ–
    print("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    create_comparison_plots(df, args)
    
    # 7. ç”ŸæˆLaTeXè¡¨æ ¼
    print("\nğŸ“ ç”ŸæˆLaTeXè¡¨æ ¼...")
    generate_latex_table(df, args)
    
    # 8. ç»Ÿè®¡åˆ†æ
    print("\n" + "="*80)
    print("ğŸ“ˆ ç»Ÿè®¡åˆ†æ")
    print("="*80)
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    print("\nğŸ“Š æŒ‰ç±»åˆ«ç»Ÿè®¡:")
    for category in df['Category'].unique():
        category_df = df[df['Category'] == category]
        print(f"\n   ã€{category}ã€‘")
        print(f"   - å®éªŒæ•°é‡: {len(category_df)}")
        print(f"   - æœ€ä½³Loss: {category_df['Best_Val_Loss'].min():.6f}")
        print(f"   - å¹³å‡Loss: {category_df['Best_Val_Loss'].mean():.6f}")
        print(f"   - æœ€å·®Loss: {category_df['Best_Val_Loss'].max():.6f}")
    
    # æŸ¥æ‰¾baselineå’Œfullæ¨¡å‹
    baseline_mask = (df['Category'].str.contains('baseline', case=False, na=False) | 
                     df['Experiment'].str.contains('Baseline', case=False, na=False))
    full_mask = (df['Category'].str.contains('full', case=False, na=False) | 
                 df['Experiment'].str.contains('Full', case=False, na=False))
    
    baseline_loss = df[baseline_mask]['Best_Val_Loss'].values
    full_loss = df[full_mask]['Best_Val_Loss'].values
    
    print("\nğŸ“Š å¯¹æ¯”åˆ†æ:")
    if len(baseline_loss) > 0 and len(full_loss) > 0:
        print(f"   Baseline Loss: {baseline_loss[0]:.6f}")
        print(f"   Full Model Loss: {full_loss[0]:.6f}")
        improvement = ((baseline_loss[0] - full_loss[0]) / baseline_loss[0] * 100)
        print(f"   æ”¹è¿›å¹…åº¦: {improvement:.2f}%")
    else:
        if len(baseline_loss) == 0:
            print("   âš ï¸  æœªæ‰¾åˆ°Baselineæ¨¡å‹")
        if len(full_loss) == 0:
            print("   âš ï¸  æœªæ‰¾åˆ°Fullæ¨¡å‹")
    
    print(f"\nğŸ“Š å…¨å±€ç»Ÿè®¡:")
    print(f"   æœ€ä½³æ¨¡å‹: {df.iloc[0]['Category']}/{df.iloc[0]['Experiment']}")
    print(f"   æœ€ä½³Loss: {df.iloc[0]['Best_Val_Loss']:.6f}")
    print(f"   æœ€å·®æ¨¡å‹: {df.iloc[-1]['Category']}/{df.iloc[-1]['Experiment']}")
    print(f"   æœ€å·®Loss: {df.iloc[-1]['Best_Val_Loss']:.6f}")
    print(f"   LossèŒƒå›´: [{df['Best_Val_Loss'].min():.6f}, {df['Best_Val_Loss'].max():.6f}]")
    print(f"   Lossæ ‡å‡†å·®: {df['Best_Val_Loss'].std():.6f}")
    
    # è¯¦ç»†æŒ‡æ ‡ç»Ÿè®¡
    print(f"\nğŸ“Š æå–çš„æŒ‡æ ‡ç»Ÿè®¡:")
    print(f"   è¯¦ç»†ç»“æœè¡¨æ ¼åˆ—æ•°: {len(detailed_df.columns)}")
    numeric_cols = detailed_df.select_dtypes(include=[np.number]).columns
    print(f"   æ•°å€¼å‹æŒ‡æ ‡æ•°é‡: {len(numeric_cols)}")
    print(f"   æ•°å€¼å‹æŒ‡æ ‡åˆ—è¡¨: {list(numeric_cols)}")
    
    print("\n" + "="*80)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("="*80 + "\n")
    
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   - {csv_file} (åŸºç¡€ç»“æœ)")
    print(f"   - {detailed_csv_file} (è¯¦ç»†ç»“æœï¼ŒåŒ…å«æ‰€æœ‰æå–çš„æŒ‡æ ‡)")
    print(f"   - {keys_file} (checkpoint keysä¿¡æ¯)")
    print(f"   - {output_path / args.latex_file} (LaTeXè¡¨æ ¼)")
    print(f"   - {output_path / 'loss_comparison.png'} (Losså¯¹æ¯”å›¾)")
    print(f"   - {output_path / 'loss_by_category.png'} (æŒ‰ç±»åˆ«Losså¯¹æ¯”å›¾)")
    if (output_path / 'multi_metric_comparison.png').exists():
        print(f"   - {output_path / 'multi_metric_comparison.png'} (å¤šæŒ‡æ ‡å¯¹æ¯”å›¾)")
    print()


if __name__ == '__main__':
    main()