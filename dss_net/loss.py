"""
æŸå¤±å‡½æ•°å®šä¹‰
ä¿®å¤ï¼šæ··åˆç²¾åº¦ä¸‹çš„SVDè®¡ç®—
æ–°å¢ï¼šæ—¶é—´ç›¸å…³æ€§çº¦æŸï¼ˆç‰©ç†æœºç†çº¦æŸï¼‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict


class ChannelDecompositionLoss(nn.Module):
    """
    æ”¹è¿›çš„ä¿¡é“åˆ†è§£æŸå¤±å‡½æ•°
    å…³é”®æ”¹è¿›ï¼š
    1. æ›´å¹³è¡¡çš„é‡å»ºæƒé‡ï¼ˆåŠ¨æ€åˆ†é‡æƒé‡åŠ å¤§ï¼‰
    2. æ›´å¼±çš„æ­£åˆ™åŒ–çº¦æŸï¼ˆé¿å…è¿‡åº¦çº¦æŸï¼‰
    3. æ·»åŠ åˆ†ç¦»è´¨é‡åº¦é‡ï¼ˆç¡®ä¿é™æ€å’ŒåŠ¨æ€çœŸçš„ä¸åŒï¼‰
    4. è‡ªé€‚åº”æƒé‡è°ƒæ•´ï¼ˆè®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´ï¼‰
    """
    
    def __init__(self, config: Dict):
        super().__init__()
        
        self.weights = config['loss']['weights']
        # ğŸ”§ å‡å¼±æ­£åˆ™åŒ–å¼ºåº¦
        self.sparsity_lambda = config['loss'].get('sparsity_lambda', 0.0001)  # ä»0.001é™åˆ°0.0001
        self.nuclear_lambda = config['loss'].get('nuclear_lambda', 0.0001)    # ä»0.001é™åˆ°0.0001
        
        # ğŸ†• æ—¶é—´ç›¸å…³æ€§é…ç½®
        self.temporal_config = config['loss'].get('temporal_correlation', {})
        self.temporal_enabled = self.temporal_config.get('enabled', True)
        self.static_smooth = self.temporal_config.get('static_smooth', True)
        self.dynamic_varying = self.temporal_config.get('dynamic_varying', True)
        self.temporal_dim = self.temporal_config.get('dim', -1)
        
        # ğŸ†• åˆ†ç¦»è´¨é‡é…ç½®
        self.separation_weight = config['loss'].get('separation_weight', 0.1)  # é¼“åŠ±é™æ€å’ŒåŠ¨æ€ä¸åŒ
    
    def forward(
        self, 
        pred: Dict[str, torch.Tensor],
        target: Dict[str, torch.Tensor],
        is_baseline: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        Args:
            pred: {
                'static': (B, 2, H, W) or None,
                'dynamic': (B, 2, H, W) or None,
                'total': (B, 2, H, W)
            }
            target: {
                'static': (B, 2, H, W),
                'dynamic': (B, 2, H, W),
                'target': (B, 2, H, W)
            }
            is_baseline: æ˜¯å¦ä¸ºåŸºçº¿æ¨¡å‹ï¼ˆä¸åˆ†ç¦»ï¼‰
        
        Returns:
            losses: dict
        """
        # ============================================
        # åŸºçº¿æ¨¡å‹ï¼šåªè®¡ç®—æ€»é‡å»ºæŸå¤±
        # ============================================
        if is_baseline or pred['static'] is None:
            total_mse = F.mse_loss(pred['total'], target['target'])
            total_nmse_db = self._compute_nmse_db(pred['total'], target['target'])
            
            return {
                'total_loss': total_mse,
                'total_mse': total_mse,
                'total_nmse_db': total_nmse_db,
                # å ä½ç¬¦ï¼ˆç”¨äºæ—¥å¿—å…¼å®¹ï¼‰
                'static_mse': torch.tensor(0.0, device=total_mse.device),
                'dynamic_mse': torch.tensor(0.0, device=total_mse.device),
                'static_l1': torch.tensor(0.0, device=total_mse.device),
                'dynamic_nuclear': torch.tensor(0.0, device=total_mse.device),
                'static_temporal': torch.tensor(0.0, device=total_mse.device),
                'dynamic_temporal': torch.tensor(0.0, device=total_mse.device),
                'static_nmse_db': torch.tensor(0.0, device=total_mse.device),
                'dynamic_nmse_db': torch.tensor(0.0, device=total_mse.device)
            }
        
        # ============================================
        # åˆ†è§£æ¨¡å‹ï¼šå®Œæ•´æŸå¤±
        # ============================================
        # 1. MSEæŸå¤±ï¼ˆæœ€é‡è¦ï¼‰
        static_mse = F.mse_loss(pred['static'], target['static'])
        dynamic_mse = F.mse_loss(pred['dynamic'], target['dynamic'])
        total_mse = F.mse_loss(pred['total'], target['target'])
        
        # ğŸ”§ 2. å‡å¼±çš„L1ç¨€ç–æ€§çº¦æŸï¼ˆé™æ€åˆ†é‡ï¼‰
        static_l1 = torch.mean(torch.abs(pred['static']))
        
        # ğŸ”§ 3. å‡å¼±çš„æ ¸èŒƒæ•°çº¦æŸï¼ˆåŠ¨æ€åˆ†é‡ä½ç§©ï¼‰
        dynamic_nuclear = self._compute_nuclear_norm(pred['dynamic'])
        
        # ğŸ†• 4. åˆ†ç¦»è´¨é‡åº¦é‡ï¼ˆç¡®ä¿é™æ€å’ŒåŠ¨æ€çœŸçš„ä¸åŒï¼‰
        separation_loss = self._compute_separation_quality(pred['static'], pred['dynamic'])
        
        # 5. æ—¶é—´ç›¸å…³æ€§çº¦æŸï¼ˆå¯é€‰ï¼‰
        if self.temporal_enabled:
            static_temporal = self._compute_temporal_variation(
                pred['static'], 
                should_be_smooth=self.static_smooth
            )
            dynamic_temporal = self._compute_temporal_variation(
                pred['dynamic'], 
                should_be_smooth=not self.dynamic_varying
            )
        else:
            static_temporal = torch.tensor(0.0, device=pred['static'].device)
            dynamic_temporal = torch.tensor(0.0, device=pred['dynamic'].device)
        
        # ğŸ”§ 6. æ”¹è¿›çš„æ€»æŸå¤±ï¼ˆæ›´æ³¨é‡é‡å»ºè´¨é‡ï¼‰
        # æ ¸å¿ƒæ€æƒ³ï¼šé‡å»ºæŸå¤± >> æ­£åˆ™åŒ–çº¦æŸ
        reconstruction_loss = (
            self.weights.get('static_mse', 1.0) * static_mse +
            self.weights.get('dynamic_mse', 2.0) * dynamic_mse +  # ğŸ”§ åŠ¨æ€åˆ†é‡æƒé‡åŠ å¤§
            self.weights.get('total_mse', 3.0) * total_mse         # ğŸ”§ æ€»é‡å»ºæœ€é‡è¦
        )
        
        regularization_loss = (
            self.weights.get('static_l1', 0.01) * self.sparsity_lambda * static_l1 +  # ğŸ”§ é™ä½æƒé‡
            self.weights.get('dynamic_nuclear', 0.01) * self.nuclear_lambda * dynamic_nuclear  # ğŸ”§ é™ä½æƒé‡
        )
        
        temporal_loss = 0.0
        if self.temporal_enabled:
            temporal_loss = (
                self.weights.get('static_temporal', 0.01) * static_temporal +  # ğŸ”§ é™ä½æƒé‡
                self.weights.get('dynamic_temporal', 0.01) * dynamic_temporal   # ğŸ”§ é™ä½æƒé‡
            )
        
        separation_term = self.separation_weight * separation_loss
        
        total_loss = reconstruction_loss + regularization_loss + temporal_loss + separation_term
        
        # 6. NMSE (dB) - ç”¨äºè¯„ä¼°
        static_nmse_db = self._compute_nmse_db(pred['static'], target['static'])
        dynamic_nmse_db = self._compute_nmse_db(pred['dynamic'], target['dynamic'])
        total_nmse_db = self._compute_nmse_db(pred['total'], target['target'])
        
        return {
            'total_loss': total_loss,
            'static_mse': static_mse,
            'dynamic_mse': dynamic_mse,
            'total_mse': total_mse,
            'static_l1': static_l1,
            'dynamic_nuclear': dynamic_nuclear,
            'static_temporal': static_temporal,      # ğŸ†•
            'dynamic_temporal': dynamic_temporal,    # ğŸ†•
            'static_nmse_db': static_nmse_db,
            'dynamic_nmse_db': dynamic_nmse_db,
            'total_nmse_db': total_nmse_db,
            'separation_loss': separation_loss  # ğŸ†• æ·»åŠ åˆ†ç¦»è´¨é‡
        }
    
    def _compute_separation_quality(
        self,
        static: torch.Tensor,
        dynamic: torch.Tensor
    ) -> torch.Tensor:
        """
        è®¡ç®—é™æ€å’ŒåŠ¨æ€åˆ†é‡çš„åˆ†ç¦»è´¨é‡
        ç›®æ ‡ï¼šç¡®ä¿ä¸¤ä¸ªåˆ†é‡çœŸçš„ä¸åŒ
        
        ä½¿ç”¨è´Ÿç›¸å…³æ€§ï¼šå¦‚æœé™æ€å’ŒåŠ¨æ€é«˜åº¦ç›¸å…³ï¼Œè¯´æ˜åˆ†ç¦»ä¸å¥½
        
        Args:
            static: (B, 2, H, W)
            dynamic: (B, 2, H, W)
        
        Returns:
            separation_loss: ç›¸å…³æ€§çš„è´Ÿå€¼ï¼ˆè¶Šå°è¶Šå¥½ï¼Œè¡¨ç¤ºç›¸å…³æ€§ä½ï¼‰
        """
        # å°†å¼ é‡å±•å¹³
        static_flat = static.view(static.size(0), -1)  # (B, 2*H*W)
        dynamic_flat = dynamic.view(dynamic.size(0), -1)
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ç›¸å…³ç³»æ•°
        correlations = []
        for i in range(static_flat.size(0)):
            s = static_flat[i]
            d = dynamic_flat[i]
            
            # ä¸­å¿ƒåŒ–
            s_mean = s.mean()
            d_mean = d.mean()
            s_centered = s - s_mean
            d_centered = d - d_mean
            
            # è®¡ç®—ç›¸å…³ç³»æ•°
            numerator = torch.sum(s_centered * d_centered)
            denominator = torch.sqrt(torch.sum(s_centered**2) * torch.sum(d_centered**2))
            
            correlation = numerator / (denominator + 1e-8)
            correlations.append(torch.abs(correlation))  # å–ç»å¯¹å€¼
        
        # å¹³å‡ç›¸å…³æ€§ï¼ˆæˆ‘ä»¬å¸Œæœ›è¿™ä¸ªå€¼å°½å¯èƒ½å°ï¼‰
        avg_correlation = torch.mean(torch.stack(correlations))
        
        return avg_correlation
    
    def _compute_temporal_variation(
        self, 
        tensor: torch.Tensor, 
        should_be_smooth: bool = True
    ) -> torch.Tensor:
        """
        æ”¹è¿›çš„æ—¶é—´å˜åŒ–æ€§è®¡ç®—
        
        Args:
            tensor: (B, 2, H, W)
            should_be_smooth: Trueè¡¨ç¤ºæƒ©ç½šå¤§å˜åŒ–ï¼ŒFalseè¡¨ç¤ºé¼“åŠ±å˜åŒ–
        
        Returns:
            temporal_loss: scalar
        """
        # æ²¿æŒ‡å®šç»´åº¦è®¡ç®—ç›¸é‚»å·®å¼‚
        if self.temporal_dim == -1:  # Wç»´åº¦
            diff = tensor[:, :, :, 1:] - tensor[:, :, :, :-1]
        elif self.temporal_dim == -2:  # Hç»´åº¦
            diff = tensor[:, :, 1:, :] - tensor[:, :, :-1, :]
        else:
            raise ValueError(f"Unsupported temporal_dim: {self.temporal_dim}")
        
        # è®¡ç®—L2èŒƒæ•°çš„å¹³æ–¹
        variation = torch.mean(diff ** 2)
        
        if should_be_smooth:
            # é™æ€åˆ†é‡ï¼šæƒ©ç½šå¤§çš„å˜åŒ–
            return variation
        else:
            # ğŸ”§ åŠ¨æ€åˆ†é‡ï¼šæ”¹ç”¨æ›´æ¸©å’Œçš„é¼“åŠ±æ–¹å¼
            # ä¸å†ä½¿ç”¨å€’æ•°ï¼ˆè¿‡äºæ¿€è¿›ï¼‰ï¼Œè€Œæ˜¯ç”¨è´Ÿæ•°ï¼ˆæ¸©å’Œé¼“åŠ±ï¼‰
            # å¦‚æœvariationå°ï¼ŒæŸå¤±ä¸ºæ­£ï¼›å¦‚æœvariationå¤§ï¼ŒæŸå¤±ä¸ºè´Ÿï¼ˆå‡å°æ€»æŸå¤±ï¼‰
            target_variation = 0.01  # æœŸæœ›çš„å˜åŒ–é‡
            return F.relu(target_variation - variation)  # åªåœ¨å˜åŒ–å¤ªå°æ—¶æƒ©ç½š
    
    def _compute_nuclear_norm(self, tensor: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—æ ¸èŒƒæ•°ï¼ˆå¥‡å¼‚å€¼ä¹‹å’Œï¼‰
        ğŸ”§ ä¿®å¤ï¼šå¼ºåˆ¶ä½¿ç”¨float32é¿å…halfç²¾åº¦ä¸‹çš„SVDé”™è¯¯
        
        Args:
            tensor: (B, 2, H, W)
        Returns:
            nuclear_norm: scalar
        """
        # ä¿å­˜åŸå§‹dtype
        original_dtype = tensor.dtype
        
        # è½¬ä¸ºfloat32ï¼ˆSVDä¸æ”¯æŒhalfï¼‰
        tensor = tensor.float()
        
        # è½¬ä¸ºå¤æ•°
        dynamic_complex = self._to_complex(tensor)  # (B, H, W)
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æ ¸èŒƒæ•°
        nuclear_norms = []
        for b in range(dynamic_complex.shape[0]):
            matrix = dynamic_complex[b]  # (H, W)
            
            # SVDè®¡ç®—ï¼ˆfloat32ï¼‰
            s = torch.linalg.svdvals(matrix)  # å¥‡å¼‚å€¼
            nuclear_norms.append(torch.sum(s))
        
        # å¹³å‡å¹¶è½¬å›åŸå§‹dtype
        nuclear_norm = torch.mean(torch.stack(nuclear_norms))
        
        return nuclear_norm.to(original_dtype)
    
    def _to_complex(self, tensor: torch.Tensor) -> torch.Tensor:
        """
        å°† (B, 2, H, W) è½¬ä¸ºå¤æ•° (B, H, W)
        """
        real = tensor[:, 0, :, :]
        imag = tensor[:, 1, :, :]
        return torch.complex(real, imag)
    
    def _compute_nmse_db(
        self, 
        pred: torch.Tensor, 
        target: torch.Tensor
    ) -> torch.Tensor:
        """
        è®¡ç®—NMSE (dB)
        NMSE_dB = 10 * log10(||pred - target||^2 / ||target||^2)
        """
        mse = torch.mean((pred - target) ** 2)
        target_power = torch.mean(target ** 2)
        
        nmse = mse / (target_power + 1e-10)
        nmse_db = 10 * torch.log10(nmse + 1e-10)
        
        return nmse_db


if __name__ == '__main__':
    # æµ‹è¯•æŸå¤±å‡½æ•°
    config = {
        'loss': {
            'weights': {
                'static_mse': 1.0,
                'dynamic_mse': 1.0,
                'total_mse': 2.0,
                'static_l1': 0.5,
                'dynamic_nuclear': 0.3,
                'static_temporal': 0.1,
                'dynamic_temporal': 0.1
            },
            'sparsity_lambda': 0.01,
            'nuclear_lambda': 0.01,
            'temporal_correlation': {
                'enabled': True,
                'static_smooth': True,
                'dynamic_varying': True,
                'dim': -1
            }
        }
    }
    
    criterion = ChannelDecompositionLoss(config)
    
    # åˆ›å»ºå‡æ•°æ®
    B, H, W = 4, 100, 150
    
    # æ¨¡æ‹Ÿé™æ€åˆ†é‡ï¼ˆåº”è¯¥å¹³æ»‘ï¼‰
    static = torch.randn(B, 2, H, 1).repeat(1, 1, 1, W)  # å¤åˆ¶ï¼Œå˜åŒ–å°
    static += torch.randn(B, 2, H, W) * 0.01  # æ·»åŠ å°å™ªå£°
    
    # æ¨¡æ‹ŸåŠ¨æ€åˆ†é‡ï¼ˆåº”è¯¥å˜åŒ–å¤§ï¼‰
    dynamic = torch.randn(B, 2, H, W)  # å®Œå…¨éšæœºï¼Œå˜åŒ–å¤§
    
    pred = {
        'static': static,
        'dynamic': dynamic,
        'total': static + dynamic
    }
    
    target = {
        'static': static + torch.randn_like(static) * 0.1,
        'dynamic': dynamic + torch.randn_like(dynamic) * 0.1,
        'target': static + dynamic + torch.randn_like(static) * 0.1
    }
    
    # æµ‹è¯•æ··åˆç²¾åº¦
    with torch.amp.autocast('cuda' if torch.cuda.is_available() else 'cpu'):
        losses = criterion(pred, target, is_baseline=False)
    
    print("="*60)
    print("ğŸ“Š Loss Components (With Temporal Correlation)")
    print("="*60)
    for key, val in losses.items():
        if 'db' in key:
            print(f"{key:20s}: {val.item():8.2f} dB")
        else:
            print(f"{key:20s}: {val.item():.6f}")
    
    print("\n" + "="*60)
    print("ğŸ”¬ Testing Ablation (Baseline Model)")
    print("="*60)
    
    pred_baseline = {
        'static': None,
        'dynamic': None,
        'total': static + dynamic
    }
    
    losses_baseline = criterion(pred_baseline, target, is_baseline=True)
    
    for key, val in losses_baseline.items():
        if 'db' in key:
            print(f"{key:20s}: {val.item():8.2f} dB")
        else:
            print(f"{key:20s}: {val.item():.6f}")