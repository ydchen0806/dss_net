"""
è®­ç»ƒè„šæœ¬ - æ”¹è¿›ç‰ˆæœ¬
- ä¿®å¤å•GPUæ¨¡å¼ä¸‹çš„å¤šè¿›ç¨‹é—®é¢˜
- ä¼˜åŒ–å†…å­˜ä½¿ç”¨
- æŠ‘åˆ¶ä¸å¿…è¦çš„è­¦å‘Š
- âœ¨ åœ¨checkpointä¸­ä¿å­˜éªŒè¯æŒ‡æ ‡
"""

import os
import sys

# ğŸ”§ åœ¨æ‰€æœ‰importå‰è®¾ç½®ç¯å¢ƒå˜é‡å’Œè­¦å‘ŠæŠ‘åˆ¶
os.environ['PYTHONWARNINGS'] = 'ignore'
os.environ['UCX_LOG_LEVEL'] = 'error'
os.environ['NCCL_DEBUG'] = 'WARN'

import warnings
warnings.filterwarnings('ignore')
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)

import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter

from pathlib import Path
from tqdm import tqdm
import numpy as np
from datetime import datetime

# DDPç›¸å…³
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from dataset import create_dataloaders
from model import UNetDecomposer, UNetBaseline, count_parameters
from loss import ChannelDecompositionLoss
from visualization import (
    create_comparison_grid,
    create_error_histogram,
    create_temporal_variation_plot
)

class Trainer:
    """è®­ç»ƒå™¨ - æ”¯æŒDDP + æ¶ˆèå®éªŒ + éªŒè¯æŒ‡æ ‡è®°å½•"""
    
    def __init__(self, config: dict, rank: int = 0, world_size: int = 1):
        """
        Args:
            config: é…ç½®å­—å…¸ï¼ˆç›´æ¥ä¼ é€’ï¼Œä¸é€šè¿‡pickleï¼‰
            rank: è¿›ç¨‹rank
            world_size: æ€»è¿›ç¨‹æ•°
        """
        self.rank = rank
        self.world_size = world_size
        self.config = config
        
        # ğŸ†• åªæœ‰world_size > 1æ—¶æ‰ä½¿ç”¨DDP
        self.use_ddp = world_size > 1 and self.config['hardware'].get('use_ddp', False)
        
        # è®¾ç½®éšæœºç§å­
        self._set_seed(self.config['experiment']['seed'] + rank)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆåªåœ¨rank 0ï¼‰
        self._setup_output_dirs()
        
        # ç­‰å¾…rank 0åˆ›å»ºå®Œç›®å½•
        if self.use_ddp:
            dist.barrier()
        
        # è®¾ç½®è®¾å¤‡
        if self.use_ddp:
            self.device = torch.device(f'cuda:{rank}')
            torch.cuda.set_device(self.device)
        else:
            # ğŸ†• å•GPUæ¨¡å¼ï¼šä½¿ç”¨CUDA_VISIBLE_DEVICESæŒ‡å®šçš„è®¾å¤‡
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        if rank == 0:
            print(f"\nğŸš€ Training Setup:")
            if self.use_ddp:
                print(f"   Mode: DDP with {world_size} GPUs")
            else:
                print(f"   Mode: Single GPU")
            print(f"   Device: {self.device}")
            if torch.cuda.is_available():
                print(f"   GPU: {torch.cuda.get_device_name(self.device)}")
        
        # ğŸ”§ æ•°æ®åŠ è½½å™¨åˆ›å»º
        if rank == 0:
            print("\n" + "="*80)
            print("ğŸ“¦ Creating Dataloaders...")
            print("="*80)
        
        # ğŸ†• ä¼ é€’use_ddpå‚æ•°ï¼Œåªåœ¨çœŸæ­£ä½¿ç”¨DDPæ—¶æ‰ä½¿ç”¨DistributedSampler
        self.train_loader, self.val_loader, self.test_loader = \
            create_dataloaders(self.config, rank=rank, world_size=world_size, use_ddp=self.use_ddp)
        
        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
        if self.use_ddp:
            dist.barrier()
        
        # åˆ›å»ºæ¨¡å‹
        if rank == 0:
            print("\n" + "="*80)
            print("ğŸ—ï¸  Building Model...")
            print("="*80)
        
        self.model = self._build_model()
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        self.criterion = ChannelDecompositionLoss(self.config)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = self._build_optimizer()
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = self._build_scheduler()
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = torch.amp.GradScaler('cuda') \
            if self.config['hardware']['use_amp'] and torch.cuda.is_available() else None
        
        # TensorBoard
        if rank == 0 and self.config['logging']['tensorboard']:
            self.writer = SummaryWriter(self.log_dir)
        else:
            self.writer = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        
        # âœ¨ æ–°å¢ï¼šè®°å½•æœ€ä½³éªŒè¯æŒ‡æ ‡
        self.best_val_metrics = {}
        
        # âœ¨ æ–°å¢ï¼šè®°å½•è®­ç»ƒå†å²
        self.train_history = {
            'train_loss': [],
            'val_loss': [],
            'train_metrics': [],
            'val_metrics': []
        }
        
        if rank == 0:
            print("\nâœ… Trainer initialized successfully!\n")
    
    def _set_seed(self, seed: int):
        """è®¾ç½®éšæœºç§å­"""
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        np.random.seed(seed)
    
    def _setup_output_dirs(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        exp_name = self.config['experiment']['name']
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ„å»ºå®éªŒæ ‡è¯†
        model_name = self.config['model']['name']
        is_ablation = self.config['model'].get('ablation', {}).get('enabled', False)
        temporal_enabled = self.config['loss'].get('temporal_correlation', {}).get('enabled', True)
        
        if is_ablation or model_name == 'UNetBaseline':
            exp_suffix = f"{model_name}_ablation"
        else:
            exp_suffix = model_name
            if not temporal_enabled:
                exp_suffix += "_no_temporal"
        
        self.exp_dir = Path(self.config['experiment']['output_dir']) / f"{exp_name}_{exp_suffix}_{timestamp}"
        self.checkpoint_dir = self.exp_dir / "checkpoints"
        self.log_dir = self.exp_dir / "logs"
        
        if self.rank == 0:
            self.exp_dir.mkdir(parents=True, exist_ok=True)
            self.checkpoint_dir.mkdir(exist_ok=True)
            self.log_dir.mkdir(exist_ok=True)
            
            # ä¿å­˜é…ç½®
            with open(self.exp_dir / "config.yaml", 'w') as f:
                yaml.dump(self.config, f, default_flow_style=False)
            
            experiment_info = {
                'experiment_name': exp_name,
                'model_name': model_name,
                'is_ablation': is_ablation,
                'temporal_enabled': temporal_enabled,
                'timestamp': timestamp,
                'output_dir': str(self.exp_dir)
            }
            
            with open(self.exp_dir / "experiment_info.yaml", 'w') as f:
                yaml.dump(experiment_info, f, default_flow_style=False)
    
    def _build_model(self) -> nn.Module:
        """æ„å»ºæ¨¡å‹"""
        model_name = self.config['model']['name']
        is_ablation = self.config['model'].get('ablation', {}).get('enabled', False)
        
        if is_ablation or model_name == 'UNetBaseline':
            if self.rank == 0:
                print("ğŸ“Š Building Baseline Model (No Decomposition - Ablation Study)")
            
            model = UNetBaseline(
                in_channels=self.config['model']['in_channels'],
                out_channels=self.config['model']['in_channels'],
                base_channels=self.config['model']['base_channels'],
                depth=self.config['model']['depth'],
                norm_type=self.config['model']['norm_type'],
                dropout=self.config['model']['dropout']
            )
            self.is_baseline = True
        
        elif model_name == 'UNetDecomposer':
            if self.rank == 0:
                print("ğŸ”¬ Building Decomposition Model (Static + Dynamic)")
            
            model = UNetDecomposer(
                in_channels=self.config['model']['in_channels'],
                base_channels=self.config['model']['base_channels'],
                depth=self.config['model']['depth'],
                norm_type=self.config['model']['norm_type'],
                dropout=self.config['model']['dropout'],
                use_attention=self.config['model'].get('use_attention', False)  # ğŸ†• æ”¯æŒattention
            )
            self.is_baseline = False
        
        else:
            raise ValueError(f"Unknown model: {model_name}")
        
        model = model.to(self.device)
        
        if self.rank == 0:
            count_parameters(model)
            
            temporal_enabled = self.config['loss'].get('temporal_correlation', {}).get('enabled', True)
            if not self.is_baseline:
                if temporal_enabled:
                    print("âœ… Temporal correlation constraints: ENABLED")
                else:
                    print("âš ï¸  Temporal correlation constraints: DISABLED (ablation)")
        
        # ğŸ†• åªåœ¨çœŸæ­£ä½¿ç”¨DDPæ—¶æ‰åŒ…è£…
        if self.use_ddp:
            model = DDP(
                model, 
                device_ids=[self.rank],
                output_device=self.rank,
                find_unused_parameters=False
            )
        
        return model
    
    def _build_optimizer(self) -> optim.Optimizer:
        """æ„å»ºä¼˜åŒ–å™¨"""
        if self.config['training']['optimizer'].lower() == 'adamw':
            optimizer = optim.AdamW(
                self.model.parameters(),
                lr=self.config['training']['learning_rate'],
                weight_decay=self.config['training']['weight_decay']
            )
        elif self.config['training']['optimizer'].lower() == 'adam':
            optimizer = optim.Adam(
                self.model.parameters(),
                lr=self.config['training']['learning_rate'],
                weight_decay=self.config['training']['weight_decay']
            )
        else:
            raise ValueError(f"Unknown optimizer: {self.config['training']['optimizer']}")
        
        return optimizer
    
    def _build_scheduler(self):
        """æ„å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        scheduler_config = self.config['training']['scheduler']
        
        if scheduler_config['type'] == 'cosine':
            scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config['training']['epochs'],
                eta_min=scheduler_config['min_lr']
            )
        elif scheduler_config['type'] == 'step':
            scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=30,
                gamma=0.1
            )
        elif scheduler_config['type'] == 'plateau':
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                patience=10,
                factor=0.5
            )
        else:
            scheduler = None
        
        return scheduler
    
    def train_epoch(self, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        # DDP: è®¾ç½®epochç”¨äºsamplerçš„shuffle
        if self.use_ddp and hasattr(self.train_loader.sampler, 'set_epoch'):
            self.train_loader.sampler.set_epoch(epoch)
        
        total_loss = 0
        metrics = {
            'static_mse': 0,
            'dynamic_mse': 0,
            'total_mse': 0,
            'static_temporal': 0,
            'dynamic_temporal': 0,
            'static_nmse_db': 0,
            'dynamic_nmse_db': 0,
            'total_nmse_db': 0
        }
        
        if self.rank == 0:
            print(f"\nğŸ”„ Starting epoch {epoch}, total batches: {len(self.train_loader)}")
        
        pbar = tqdm(
            self.train_loader, 
            desc=f"Epoch {epoch}/{self.config['training']['epochs']}",
            disable=(self.rank != 0)
        )
        
        for batch_idx, batch in enumerate(pbar):
            # ğŸ”§ æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼ˆä»…ç¬¬ä¸€ä¸ªbatchï¼‰
            if batch_idx == 0 and self.rank == 0:
                print(f"\nâœ… æˆåŠŸåŠ è½½ç¬¬ä¸€ä¸ªbatch")
            
            # æ•°æ®è½¬ç§»åˆ°è®¾å¤‡
            inputs = batch['input'].to(self.device, non_blocking=True)
            target_static = batch['static'].to(self.device, non_blocking=True)
            target_dynamic = batch['dynamic'].to(self.device, non_blocking=True)
            target_total = batch['target'].to(self.device, non_blocking=True)
            
            # ğŸ”§ ç¬¬ä¸€ä¸ªbatchçš„è°ƒè¯•ä¿¡æ¯
            if batch_idx == 0 and self.rank == 0:
                print(f"âœ… æ•°æ®å·²ä¼ è¾“åˆ°GPU")
                print(f"   Input shape: {inputs.shape}")
            
            # å‰å‘ä¼ æ’­
            if self.scaler is not None:
                with torch.amp.autocast('cuda'):
                    pred = self.model(inputs)
                    
                    losses = self.criterion(
                        pred,
                        {
                            'static': target_static,
                            'dynamic': target_dynamic,
                            'target': target_total
                        },
                        is_baseline=self.is_baseline
                    )
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                self.scaler.scale(losses['total_loss']).backward()
                
                # æ¢¯åº¦è£å‰ª
                if self.config['training'].get('gradient_clip'):
                    self.scaler.unscale_(self.optimizer)
                    nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.config['training']['gradient_clip']
                    )
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
            
            else:
                pred = self.model(inputs)
                
                losses = self.criterion(
                    pred,
                    {
                        'static': target_static,
                        'dynamic': target_dynamic,
                        'target': target_total
                    },
                    is_baseline=self.is_baseline
                )
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                losses['total_loss'].backward()
                
                # æ¢¯åº¦è£å‰ª
                if self.config['training'].get('gradient_clip'):
                    nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.config['training']['gradient_clip']
                    )
                
                self.optimizer.step()
            
            # ç´¯ç§¯æŒ‡æ ‡
            total_loss += losses['total_loss'].item()
            for key in metrics:
                if key in losses:
                    metrics[key] += losses[key].item()
            
            # æ›´æ–°è¿›åº¦æ¡
            if self.rank == 0:
                postfix_dict = {
                    'loss': losses['total_loss'].item(),
                    'total_db': losses['total_nmse_db'].item()
                }
                
                if not self.is_baseline:
                    postfix_dict.update({
                        'static_db': losses['static_nmse_db'].item(),
                        'dynamic_db': losses['dynamic_nmse_db'].item(),
                    })
                    
                    if self.config['loss'].get('temporal_correlation', {}).get('enabled', True):
                        postfix_dict.update({
                            's_temp': losses['static_temporal'].item(),
                            'd_temp': losses['dynamic_temporal'].item()
                        })
                
                pbar.set_postfix(postfix_dict)
            
            # TensorBoardæ—¥å¿—
            if self.rank == 0 and self.writer is not None:
                global_step = epoch * len(self.train_loader) + batch_idx
                
                if batch_idx % self.config['logging']['log_interval'] == 0:
                    self.writer.add_scalar('Train/Loss', losses['total_loss'].item(), global_step)
                    self.writer.add_scalar('Train/Total_NMSE_dB', losses['total_nmse_db'].item(), global_step)
                    
                    if not self.is_baseline:
                        self.writer.add_scalar('Train/Static_NMSE_dB', losses['static_nmse_db'].item(), global_step)
                        self.writer.add_scalar('Train/Dynamic_NMSE_dB', losses['dynamic_nmse_db'].item(), global_step)
                        
                        if self.config['loss'].get('temporal_correlation', {}).get('enabled', True):
                            self.writer.add_scalar('Train/Static_Temporal', losses['static_temporal'].item(), global_step)
                            self.writer.add_scalar('Train/Dynamic_Temporal', losses['dynamic_temporal'].item(), global_step)
            
            # ğŸ†• æ¸…ç†ç¼“å­˜
            if batch_idx % 50 == 0 and torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        # å¹³å‡æŒ‡æ ‡
        avg_loss = total_loss / len(self.train_loader)
        avg_metrics = {k: v / len(self.train_loader) for k, v in metrics.items()}
        
        # DDP: åŒæ­¥æŒ‡æ ‡
        if self.use_ddp:
            avg_loss = self._reduce_value(avg_loss)
            avg_metrics = {k: self._reduce_value(v) for k, v in avg_metrics.items()}
        
        return avg_loss, avg_metrics
    
    @torch.no_grad()
    def validate(self, epoch: int):
        """éªŒè¯"""
        self.model.eval()
        
        total_loss = 0
        metrics = {
            'static_mse': 0,
            'dynamic_mse': 0,
            'total_mse': 0,
            'static_temporal': 0,
            'dynamic_temporal': 0,
            'static_nmse_db': 0,
            'dynamic_nmse_db': 0,
            'total_nmse_db': 0
        }
        
        pbar = tqdm(
            self.val_loader,
            desc="Validating",
            disable=(self.rank != 0)
        )
        
        for batch in pbar:
            inputs = batch['input'].to(self.device, non_blocking=True)
            target_static = batch['static'].to(self.device, non_blocking=True)
            target_dynamic = batch['dynamic'].to(self.device, non_blocking=True)
            target_total = batch['target'].to(self.device, non_blocking=True)
            
            pred = self.model(inputs)
            
            losses = self.criterion(
                pred,
                {
                    'static': target_static,
                    'dynamic': target_dynamic,
                    'target': target_total
                },
                is_baseline=self.is_baseline
            )
            
            total_loss += losses['total_loss'].item()
            for key in metrics:
                if key in losses:
                    metrics[key] += losses[key].item()
        
        avg_loss = total_loss / len(self.val_loader)
        avg_metrics = {k: v / len(self.val_loader) for k, v in metrics.items()}
        
        # DDP: åŒæ­¥æŒ‡æ ‡
        if self.use_ddp:
            avg_loss = self._reduce_value(avg_loss)
            avg_metrics = {k: self._reduce_value(v) for k, v in avg_metrics.items()}
        
        # TensorBoard
        if self.rank == 0 and self.writer is not None:
            self.writer.add_scalar('Val/Loss', avg_loss, epoch)
            self.writer.add_scalar('Val/Total_NMSE_dB', avg_metrics['total_nmse_db'], epoch)
            
            if not self.is_baseline:
                self.writer.add_scalar('Val/Static_NMSE_dB', avg_metrics['static_nmse_db'], epoch)
                self.writer.add_scalar('Val/Dynamic_NMSE_dB', avg_metrics['dynamic_nmse_db'], epoch)
                
                if self.config['loss'].get('temporal_correlation', {}).get('enabled', True):
                    self.writer.add_scalar('Val/Static_Temporal', avg_metrics['static_temporal'], epoch)
                    self.writer.add_scalar('Val/Dynamic_Temporal', avg_metrics['dynamic_temporal'], epoch)
            
            # ğŸ†• å¯è§†åŒ–ï¼šæ ¹æ®é…ç½®å†³å®šæ˜¯å¦è®°å½•å›¾åƒ
            vis_config = self.config['logging'].get('visualization', {})
            if vis_config.get('enabled', True):
                vis_interval = vis_config.get('interval', 5)
                if epoch % vis_interval == 0:
                    self._log_visualizations(epoch)
        
        return avg_loss, avg_metrics
    
    def _reduce_value(self, value: float) -> float:
        """DDP: è·¨è¿›ç¨‹å¹³å‡"""
        if not self.use_ddp:
            return value
        
        tensor = torch.tensor(value, device=self.device)
        dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
        return tensor.item() / self.world_size
    
    @torch.no_grad()
    def _log_visualizations(self, epoch: int):
        """
        è®°å½•å¯è§†åŒ–å›¾åƒåˆ°TensorBoard
        
        Args:
            epoch: å½“å‰epoch
        """
        if self.rank != 0 or self.writer is None:
            return
        
        print(f"\nğŸ“Š Generating visualizations for epoch {epoch}...")
        
        self.model.eval()
        
        # è·å–å¯è§†åŒ–é…ç½®
        vis_config = self.config['logging'].get('visualization', {})
        num_samples = vis_config.get('num_samples', 4)
        modes = vis_config.get('modes', ['magnitude', 'phase'])
        
        # ä»éªŒè¯é›†è·å–ä¸€ä¸ªbatch
        try:
            # è·å–éªŒè¯é›†çš„ç¬¬ä¸€ä¸ªbatch
            val_iter = iter(self.val_loader)
            batch = next(val_iter)
            
            inputs = batch['input'].to(self.device)
            target_static = batch['static'].to(self.device)
            target_dynamic = batch['dynamic'].to(self.device)
            target_total = batch['target'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            pred = self.model(inputs)
            
            target = {
                'static': target_static,
                'dynamic': target_dynamic,
                'target': target_total
            }
            
            # æ ¹æ®é…ç½®åˆ›å»ºå¯¹æ¯”å›¾
            for mode in modes:
                print(f"   - Creating {mode} comparison grid...")
                try:
                    grid_img = create_comparison_grid(
                        inputs, pred, target,
                        is_baseline=self.is_baseline,
                        num_samples=min(num_samples, inputs.size(0)),
                        mode=mode
                    )
                    # è½¬æ¢ä¸ºCHWæ ¼å¼ (TensorBoardéœ€è¦)
                    grid_img = torch.from_numpy(grid_img).permute(2, 0, 1)
                    self.writer.add_image(f'Visualization/{mode.capitalize()}_Comparison', 
                                         grid_img, epoch, dataformats='CHW')
                except Exception as e:
                    print(f"   âš ï¸  Failed to create {mode} grid: {e}")
            
            # è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
            print("   - Creating error histogram...")
            try:
                hist_img = create_error_histogram(pred, target, is_baseline=self.is_baseline)
                hist_img = torch.from_numpy(hist_img).permute(2, 0, 1)
                self.writer.add_image('Visualization/Error_Histogram', 
                                     hist_img, epoch, dataformats='CHW')
            except Exception as e:
                print(f"   âš ï¸  Failed to create histogram: {e}")
            
            # æ—¶é—´å˜åŒ–å›¾ï¼ˆä»…åˆ†è§£æ¨¡å‹ï¼‰
            if not self.is_baseline:
                print("   - Creating temporal variation plot...")
                try:
                    temporal_dim = self.config['loss'].get('temporal_correlation', {}).get('dim', -1)
                    temporal_img = create_temporal_variation_plot(
                        pred, target, 
                        is_baseline=False, 
                        dim=temporal_dim
                    )
                    if temporal_img is not None:
                        temporal_img = torch.from_numpy(temporal_img).permute(2, 0, 1)
                        self.writer.add_image('Visualization/Temporal_Variation', 
                                             temporal_img, epoch, dataformats='CHW')
                except Exception as e:
                    print(f"   âš ï¸  Failed to create temporal plot: {e}")
            
            print(f"âœ… Visualizations saved to TensorBoard!")
        
        except Exception as e:
            print(f"âŒ Failed to generate visualizations: {e}")
            import traceback
            traceback.print_exc()
    
    def save_checkpoint(self, epoch: int, val_loss: float, val_metrics: dict, is_best: bool = False):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹ - âœ¨ å¢åŠ éªŒè¯æŒ‡æ ‡ä¿å­˜
        
        Args:
            epoch: å½“å‰epoch
            val_loss: éªŒè¯æŸå¤±
            val_metrics: éªŒè¯æŒ‡æ ‡å­—å…¸
            is_best: æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        """
        if self.rank != 0:
            return
        
        # DDP: ä¿å­˜æ¨¡å‹æ—¶å»æ‰module.å‰ç¼€
        model_state = self.model.module.state_dict() if self.use_ddp else self.model.state_dict()
        
        # âœ¨ æ„å»ºå®Œæ•´çš„checkpoint
        checkpoint = {
            # æ¨¡å‹ç›¸å…³
            'epoch': epoch,
            'model_state_dict': model_state,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            
            # âœ¨ éªŒè¯æŒ‡æ ‡
            'val_loss': val_loss,
            'val_metrics': val_metrics,
            'best_val_loss': self.best_val_loss,
            'best_val_metrics': self.best_val_metrics,
            
            # âœ¨ è®­ç»ƒå†å²
            'train_history': self.train_history,
            
            # é…ç½®ä¿¡æ¯
            'config': self.config,
            'is_baseline': self.is_baseline,
            
            # âœ¨ é¢å¤–ä¿¡æ¯
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'total_epochs': self.config['training']['epochs'],
        }
        
        # ä¿å­˜æœ€æ–°
        latest_path = self.checkpoint_dir / 'latest.pth'
        torch.save(checkpoint, latest_path)
        print(f"ğŸ’¾ Saved latest checkpoint to {latest_path}")
        
        # ä¿å­˜æœ€ä½³
        if is_best:
            best_path = self.checkpoint_dir / 'best.pth'
            torch.save(checkpoint, best_path)
            print(f"ğŸ† Saved best checkpoint to {best_path}")
            print(f"   Val Loss: {val_loss:.6f}")
            print(f"   Val Total NMSE: {val_metrics['total_nmse_db']:.2f} dB")
            if not self.is_baseline:
                print(f"   Val Static NMSE: {val_metrics['static_nmse_db']:.2f} dB")
                print(f"   Val Dynamic NMSE: {val_metrics['dynamic_nmse_db']:.2f} dB")
        
        # å®šæœŸä¿å­˜
        if epoch % self.config['logging']['save_checkpoint_interval'] == 0:
            epoch_path = self.checkpoint_dir / f'epoch_{epoch:03d}.pth'
            torch.save(checkpoint, epoch_path)
            print(f"ğŸ“Œ Saved epoch checkpoint to {epoch_path}")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        if self.rank == 0:
            print("\n" + "="*80)
            print("ğŸš€ Starting Training...")
            print("="*80 + "\n")
        
        for epoch in range(1, self.config['training']['epochs'] + 1):
            self.current_epoch = epoch
            
            # è®­ç»ƒ
            train_loss, train_metrics = self.train_epoch(epoch)
            
            # âœ¨ è®°å½•è®­ç»ƒå†å²
            if self.rank == 0:
                self.train_history['train_loss'].append(train_loss)
                self.train_history['train_metrics'].append(train_metrics.copy())
            
            # éªŒè¯
            if epoch % self.config['validation']['interval'] == 0:
                val_loss, val_metrics = self.validate(epoch)
                
                # âœ¨ è®°å½•éªŒè¯å†å²
                if self.rank == 0:
                    self.train_history['val_loss'].append(val_loss)
                    self.train_history['val_metrics'].append(val_metrics.copy())
                
                # æ‰“å°ç»“æœ
                if self.rank == 0:
                    print(f"\n{'='*80}")
                    print(f"Epoch {epoch}/{self.config['training']['epochs']}")
                    print(f"{'='*80}")
                    print(f"Train Loss: {train_loss:.6f}")
                    
                    if not self.is_baseline:
                        print(f"  Static NMSE:  {train_metrics['static_nmse_db']:.2f} dB")
                        print(f"  Dynamic NMSE: {train_metrics['dynamic_nmse_db']:.2f} dB")
                        
                        if self.config['loss'].get('temporal_correlation', {}).get('enabled', True):
                            print(f"  Static Temp:  {train_metrics['static_temporal']:.6f}")
                            print(f"  Dynamic Temp: {train_metrics['dynamic_temporal']:.6f}")
                    
                    print(f"  Total NMSE:   {train_metrics['total_nmse_db']:.2f} dB")
                    print(f"\nVal Loss: {val_loss:.6f}")
                    
                    if not self.is_baseline:
                        print(f"  Static NMSE:  {val_metrics['static_nmse_db']:.2f} dB")
                        print(f"  Dynamic NMSE: {val_metrics['dynamic_nmse_db']:.2f} dB")
                        
                        if self.config['loss'].get('temporal_correlation', {}).get('enabled', True):
                            print(f"  Static Temp:  {val_metrics['static_temporal']:.6f}")
                            print(f"  Dynamic Temp: {val_metrics['dynamic_temporal']:.6f}")
                    
                    print(f"  Total NMSE:   {val_metrics['total_nmse_db']:.2f} dB")
                    print(f"{'='*80}\n")
                
                # âœ¨ ä¿å­˜æœ€ä½³æ¨¡å‹å¹¶æ›´æ–°æœ€ä½³æŒ‡æ ‡
                is_best = val_loss < self.best_val_loss
                if is_best:
                    if self.rank == 0:
                        print(f"ğŸ‰ New best model at epoch {epoch}!")
                        print(f"   Previous best loss: {self.best_val_loss:.6f}")
                        print(f"   New best loss: {val_loss:.6f}")
                        print(f"   Improvement: {self.best_val_loss - val_loss:.6f}")
                    
                    self.best_val_loss = val_loss
                    self.best_val_metrics = val_metrics.copy()  # âœ¨ ä¿å­˜æœ€ä½³æŒ‡æ ‡
                    self.patience_counter = 0
                else:
                    self.patience_counter += 1
                    if self.rank == 0:
                        print(f"â³ No improvement for {self.patience_counter} validation(s)")
                        print(f"   Best loss so far: {self.best_val_loss:.6f}")
                
                # âœ¨ ä¿å­˜checkpointï¼ˆä¼ å…¥éªŒè¯æŒ‡æ ‡ï¼‰
                self.save_checkpoint(epoch, val_loss, val_metrics, is_best)
                
                # æ—©åœ
                if self.patience_counter >= self.config['training']['early_stopping']['patience']:
                    if self.rank == 0:
                        print(f"\nâš ï¸  Early stopping triggered at epoch {epoch}")
                        print(f"   Best validation loss: {self.best_val_loss:.6f}")
                        print(f"   Best metrics:")
                        for key, value in self.best_val_metrics.items():
                            print(f"      {key}: {value:.6f}")
                    break
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler is not None:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()
                
                # è®°å½•å­¦ä¹ ç‡
                if self.rank == 0 and self.writer is not None:
                    current_lr = self.optimizer.param_groups[0]['lr']
                    self.writer.add_scalar('Train/Learning_Rate', current_lr, epoch)
        
        # âœ¨ è®­ç»ƒç»“æŸåä¿å­˜æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        if self.rank == 0:
            self._save_training_summary()
            
            print("\nâœ… Training completed!")
            print(f"ğŸ“ Results saved to: {self.exp_dir}")
            print(f"\nğŸ† Best Results:")
            print(f"   Epoch: {self.train_history['val_loss'].index(self.best_val_loss) + 1}")
            print(f"   Val Loss: {self.best_val_loss:.6f}")
            print(f"   Metrics:")
            for key, value in self.best_val_metrics.items():
                print(f"      {key}: {value:.6f}")
            print()
    
    def _save_training_summary(self):
        """âœ¨ ä¿å­˜è®­ç»ƒæ‘˜è¦"""
        if self.rank != 0:
            return
        
        summary = {
            'experiment_name': self.config['experiment']['name'],
            'model_name': self.config['model']['name'],
            'is_baseline': self.is_baseline,
            'total_epochs': self.current_epoch,
            'best_epoch': self.train_history['val_loss'].index(self.best_val_loss) + 1 if self.train_history['val_loss'] else -1,
            'best_val_loss': self.best_val_loss,
            'best_val_metrics': self.best_val_metrics,
            'final_train_loss': self.train_history['train_loss'][-1] if self.train_history['train_loss'] else None,
            'final_val_loss': self.train_history['val_loss'][-1] if self.train_history['val_loss'] else None,
            'training_completed': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        }
        
        summary_path = self.exp_dir / 'training_summary.yaml'
        with open(summary_path, 'w') as f:
            yaml.dump(summary, f, default_flow_style=False)
        
        print(f"ğŸ“Š Training summary saved to {summary_path}")

def setup_ddp(rank: int, world_size: int):
    """åˆå§‹åŒ–DDP"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    
    # åˆå§‹åŒ–è¿›ç¨‹ç»„
    dist.init_process_group(
        backend='nccl',
        init_method='env://',
        world_size=world_size,
        rank=rank
    )

def cleanup_ddp():
    """æ¸…ç†DDP"""
    if dist.is_initialized():
        dist.destroy_process_group()

def main_worker(rank: int, world_size: int, config: dict):
    """
    å•ä¸ªè¿›ç¨‹çš„å·¥ä½œå‡½æ•°
    
    Args:
        rank: è¿›ç¨‹rank
        world_size: æ€»è¿›ç¨‹æ•°
        config: é…ç½®å­—å…¸
    """
    # ğŸ†• åªåœ¨world_size > 1æ—¶åˆå§‹åŒ–DDP
    if world_size > 1:
        setup_ddp(rank, world_size)
    
    try:
        trainer = Trainer(config, rank=rank, world_size=world_size)
        trainer.train()
    
    except Exception as e:
        print(f"âŒ Rank {rank} failed with error: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†
        if world_size > 1:
            cleanup_ddp()

def main():
    import argparse
    import torch.multiprocessing as mp
    
    parser = argparse.ArgumentParser(description='Train Channel Decomposition Model')
    parser.add_argument('--config', type=str, default='config.yaml',
                        help='Path to config file')
    parser.add_argument('--gpus', type=int, default=None,
                        help='Number of GPUs to use (overrides config)')
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    # ğŸ†• ç¡®å®šGPUæ•°é‡ï¼šä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    if args.gpus is not None:
        world_size = args.gpus
    else:
        # å¦‚æœæ²¡æœ‰æŒ‡å®š--gpusï¼Œæ£€æŸ¥CUDA_VISIBLE_DEVICES
        cuda_visible = os.environ.get('CUDA_VISIBLE_DEVICES', '')
        if cuda_visible:
            # CUDA_VISIBLE_DEVICESè¢«è®¾ç½®ï¼Œè®¡ç®—å¯è§GPUæ•°é‡
            world_size = len([x for x in cuda_visible.split(',') if x.strip()])
        else:
            # ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
            world_size = torch.cuda.device_count() if config['hardware'].get('use_ddp', False) else 1
    
    # ğŸ†• å¦‚æœåªæœ‰1ä¸ªGPUï¼Œå¼ºåˆ¶å•GPUæ¨¡å¼
    if world_size == 1:
        config['hardware']['use_ddp'] = False
    
    print(f"\n{'='*80}")
    print(f"ğŸš€ Starting Training")
    print(f"{'='*80}")
    print(f"   Experiment: {config['experiment']['name']}")
    print(f"   Model: {config['model']['name']}")
    
    # æ‰“å°å®éªŒä¿¡æ¯
    is_ablation = config['model'].get('ablation', {}).get('enabled', False)
    temporal_enabled = config['loss'].get('temporal_correlation', {}).get('enabled', True)
    
    if is_ablation or config['model']['name'] == 'UNetBaseline':
        print(f"   Type: ğŸ”¬ ABLATION STUDY - Baseline (No Decomposition)")
    else:
        print(f"   Type: ğŸ† Full Model (Static + Dynamic Decomposition)")
        if temporal_enabled:
            print(f"   Temporal Constraints: âœ… ENABLED")
        else:
            print(f"   Temporal Constraints: âš ï¸  DISABLED (ablation)")
    
    print(f"   GPUs: {world_size}")
    print(f"   Config: {args.config}")
    print(f"{'='*80}\n")
    
    # ğŸ†• åªæœ‰world_size > 1æ—¶æ‰ä½¿ç”¨spawn
    if world_size > 1:
        mp.spawn(
            main_worker,
            args=(world_size, config),
            nprocs=world_size,
            join=True
        )
    else:
        # ğŸ†• å•GPUç›´æ¥è¿è¡Œï¼Œä¸ä½¿ç”¨spawn
        print("ğŸ’¡ Running in single GPU mode (no multiprocessing)\n")
        main_worker(0, 1, config)

if __name__ == '__main__':
    # ğŸ†• è®¾ç½®multiprocessingå¯åŠ¨æ–¹æ³•
    import torch.multiprocessing as mp
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        pass  # å·²ç»è®¾ç½®è¿‡äº†
    
    main()